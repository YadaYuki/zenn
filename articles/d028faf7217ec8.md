---
title: "ã€LLM for NewsRecã€‘å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(BERT)ã‚’æ´»ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®PyTorchã«ã‚ˆã‚‹å®Ÿè£…ã¨è©•ä¾¡"
emoji: "ğŸ“‘"
type: "tech"
topics: ["LLM", "PyTorch", "nlp", "BERT"]
published: true
---

# 1. ã¯ã˜ã‚ã«

ä¸–ã¯å¤§ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ™‚ä»£ã€‚ã€Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ç´™ã§ã¯ãªãã€ã‚¹ãƒãƒ›ã§ã€‚ã€ãŒå½“ãŸã‚Šå‰ã€‚æ—¥ã€…ç”Ÿã¿å‡ºã•ã‚Œã‚‹è†¨å¤§ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ä¸­ã‹ã‚‰å€‹äººã®å—œå¥½ã«åŸºã¥ã„ãŸè¨˜äº‹ã‚’æŠ½å‡ºã™ã‚‹**ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ **ã®éœ€è¦ã¯é«˜ã¾ã‚Šã€Microsoft Newsã‚„Yahoo Newsã€Smart Newsãªã©æ•°å¤šãã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¡ãƒ‡ã‚£ã‚¢ãŒã€ãã®åˆ†é‡ã«å¤šå¤§ãªã‚‹åŠ´åŠ›ã‚’å‰²ã„ã¦ã„ã¾ã™ã€‚ãã—ã¦ã€è¿‘å¹´ç”¨ã„ã‚‰ã‚Œã‚‹æ‰‹æ³•ã®å¤šãã¯æ©Ÿæ¢°å­¦ç¿’æŠ€è¡“ãŒç”¨ã„ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«ãŠã‘ã‚‹æ¨è–¦ã‚¢ã‚¤ãƒ†ãƒ ã¯ã€ã„ã†ã¾ã§ã‚‚ãªãã€Œãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã€ã€‚ãã—ã¦ãã®å¤§éƒ¨åˆ†ã¯ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ x ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã¨ãªã‚‹ã¨ã€ä»Šæœ€ã‚‚ãƒ›ãƒƒãƒˆãªãƒˆãƒ”ãƒƒã‚¯ã¨ã„ãˆã°ã€ã‚„ã¯ã‚Š**å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å¿œç”¨**ã§ã™ã€‚

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã€è†¨å¤§ãªã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚ˆã‚‹äº‹å‰å­¦ç¿’ã‚’é€šã—ã¦æ·±ã„è¨€èªç†è§£ã‚’ç²å¾—ã—ãŸå¤§è¦æ¨¡ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã™ã€‚æ–‡æ›¸åˆ†é¡ã‚„ç¿»è¨³ã€å¯¾è©±å¿œç­”ãªã©ã€æ§˜ã€…ãªè‡ªç„¶è¨€èªå‡¦ç†ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€é«˜ã„æ€§èƒ½ã‚’å‡ºã™ã“ã¨ã§çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ãã—ã¦ã€å¤šãã®å…ˆè¡Œç ”ç©¶ã§ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«ãŠã‘ã‚‹ã€Œ**ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã‚„ã€Œãƒ¦ãƒ¼ã‚¶ã€ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°(ãƒ™ã‚¯ãƒˆãƒ«åŒ–)ã«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸæ‰‹æ³•ãŒé«˜ã„æ€§èƒ½ã‚’æŒ™ã’ã¦ã„ã‚‹**ã“ã¨ãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ [1-3, 9]ã€‚

ã•ã¦ã€ãã“ã§ä»Šå›ã¯ã€[PyTorch](https://pytorch.org/)ã¨[huggingface/transformers](https://huggingface.co/)ã‚’ç”¨ã„ã¦ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦æ‰‹æ³•ã§ã‚ã‚‹**PLM-NR**(**NRMS-BERT**)[1]ã®å®Ÿè£…ãŠã‚ˆã³è©•ä¾¡ã‚’è¡Œã„ã¾ã—ãŸã€‚è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã¯**BERT-base**[4]ã¨**DistilBERT-base**[5]ã‚’ç”¨ã„ã¦ã„ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŠã‚ˆã³è©•ä¾¡ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã¯[**MIND**](https://msnews.github.io/)[3]ã¨ã„ã†Microsoftç™ºã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ã—ãŸçµæœã€**PLM-NR[1]ã®ææ¡ˆè«–æ–‡ã«è¨˜è¼‰ã•ã‚ŒãŸçµæœã«è¿«ã‚‹é«˜ã„æ€§èƒ½ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸ**ã€‚å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«é–¢ã—ã¦ã‚‚ä¸€èˆ¬å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚

ä»¥ä¸‹ã€å®Ÿè£…ã—ãŸãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚

https://github.com/YadaYuki/news-recommendation-llm

# 2. æ·±å±¤å­¦ç¿’ã‚’ç”¨ã„ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦

æ¨è–¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯ã€è¡Œåˆ—åˆ†è§£ã‚„ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ãŸæ¨è–¦ãªã©ã€æ§˜ã€…ãªæ‰‹æ³•ãŒå­˜åœ¨ã—ã¾ã™ã€‚è§£ããŸã„ã‚¿ã‚¹ã‚¯ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³ã«å¿œã˜ã¦ã€ã©ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒé©åˆ‡ã§ã‚ã‚‹ã‹ã€ã¯ã‚±ãƒ¼ã‚¹ãƒã‚¤ã‚±ãƒ¼ã‚¹ã§ã™ã€‚

ãã‚“ãªä¸­ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã§ã¯ã€**æ·±å±¤å­¦ç¿’ã‚’ç”¨ã„ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«**ãŒç‰¹ã«é«˜ã„æ€§èƒ½ã‚’å‡ºã™ã“ã¨ãŒã€è¿‘å¹´ã®å­¦è¡“ç ”ç©¶ã§ã¯çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ [1-3, 9]ã€‚

## 2.1 ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ 

ã¾ãšã¯ã€æ·±å±¤å­¦ç¿’ã‚’ç”¨ã„ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®ä¸€èˆ¬æ§‹é€ ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ä»¥ä¸‹ã¯[å…ˆè¡Œç ”ç©¶ã®è«–æ–‡](https://arxiv.org/pdf/2104.07413.pdf)ã‹ã‚‰å¼•ç”¨ã—ãŸå›³ã§ã™ã€‚

![](/images/d028faf7217ec8/news-rec-overview.png =400x)

å›³ã®$D_c$ã¯æ¨è–¦å€™è£œã¨ãªã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹(Candidate News)ã€$D_1$ ~ $D_T$ã¯æ¨è–¦ã•ã‚Œã‚‹ãƒ¦ãƒ¼ã‚¶ãŒéå»ã«èª­ã‚“ã $T$æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹(Clicked News)ã§ã™ã€‚

ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¯ã€å¤§ããã‚ã‘ã¦ã€ä»¥ä¸‹ã®3ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚

- **News Encoder**: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„($D_c$, $D_1$ ~ $D_T$)ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æ„å‘³åˆã„ã‚’åæ˜ ã—ãŸ$d$æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«($h_c$, $h_1$ ~ $h_T$)ã‚’å‡ºåŠ›ã™ã‚‹
- **User Encoder**: éå»ã«ãƒ¦ãƒ¼ã‚¶ãŒèª­ã‚“ã ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ãƒªã‚¹ãƒˆ($D_1$ ~ $D_T$)ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’åæ˜ ã—ãŸ$d$æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«($u$)ã‚’å‡ºåŠ›ã™ã‚‹

- **Click Predictor**: User Encoderã¨News Encoderã®$d$æ¬¡å…ƒã®å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«($u$,$h_c$)ã®å†…ç©$u^T\cdot h_c$ã‚’è¨ˆç®—ã—ã€ãƒ¦ãƒ¼ã‚¶ãŒ$D_c$ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ç¢ºç‡$\^y$ã‚’å‡ºåŠ›ã™ã‚‹

ã™ãªã‚ã¡ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¯ã€ã€Œ**éå»ã«ãƒ¦ãƒ¼ã‚¶ãŒèª­ã‚“ã è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½**ã€ã¨ã€Œ**æ¨è–¦å¯¾è±¡ã¨ãªã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ‡ãƒ¼ã‚¿**ã€ã‚’ãã‚Œãã‚ŒåŒã˜æ¬¡å…ƒæ•°ã®ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ã€ãã®**é¡ä¼¼åº¦ã‚’ã€å†…ç©ã«ã‚ˆã‚Šè¨ˆç®—ã™ã‚‹ã“ã¨ã§ã€ã‚¯ãƒªãƒƒã‚¯ç‡ã‚’äºˆæ¸¬ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«**ã§ã‚ã‚‹ã¨è¨€ãˆã¾ã™ã€‚

ã€Œ**ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚„ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ã©ã®ã‚ˆã†ã«ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ã®ã‹(News Encoderã¨User Encoderã‚’ã©ã†æ§‹ç¯‰ã™ã‚‹ã‹)ï¼Ÿ**ã€ã¨ã„ã†ã®ãŒãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«ãŠã‘ã‚‹æœ€ã‚‚é‡è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨è¨€ã£ã¦ã‚‚éè¨€ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

è¿‘å¹´ã§ã¯ã€ãã‚Œã‚‰ã®**ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«BERTã‚’ã¯ã˜ã‚ã¨ã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸæ‰‹æ³•**ãŒã€é«˜ã„æ€§èƒ½ã‚’å‡ºã™ã“ã¨ã§çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

## 2.2 PLM-NR [1]

2019å¹´ã®BERTã®ç™»å ´ã‚’çš®åˆ‡ã‚Šã«ã€RoBERTaã‚„GPTã€LLaMAãªã©ã€æ§˜ã€…ãªTransformerã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒææ¡ˆãƒ»å…¬é–‹ã•ã‚Œã€ã‚ã‚‰ã‚†ã‚‹è‡ªç„¶è¨€èªå‡¦ç†ã‚¿ã‚¹ã‚¯ã§é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã—ã¦ã„ã¾ã™ã€‚

ä»Šå›å®Ÿè£…ã—ãŸ**PLM-NR**(**Pre-trained Language Model empowered News Recommendation**)ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦æ‰‹æ³•ã§ã€2021å¹´ã«Wuã‚‰ã«ã‚ˆã‚Šã€ãã®çµæœãŒå ±å‘Šã•ã‚Œã¾ã—ãŸ[1]ã€‚

ã“ã®ç ”ç©¶ã§ã¯ã€2.1ç¯€ã§ã‚‚èª¬æ˜ãŒã‚ã£ãŸé€šã‚Šã€News Encoderã¨User Encoderã«**BERT**ã‚’ç”¨ã„ã¦ã„ã¾ã™ã€‚å½¼ã‚‰ã¯News Encoderã¨User Encoderã«BERTã‚’ç”¨ã„ãŸè¤‡æ•°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€å¾Œè¿°ã™ã‚‹MIND[3]ã‚’ç”¨ã„ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¤œè¨¼ã‚’è¡Œã„ã€**ãã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ãŒé«˜ã„æ€§èƒ½ãŒå‡ºã™ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸ**ã€‚

å½¼ã‚‰ã®å ±å‘Šã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¤œè¨¼ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡ã«ç•™ã¾ã‚Šã¾ã›ã‚“ã€‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã¨ã—ã¦ã€PLM-NRã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿéš›ã«[Microsoft Newsãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ](https://news.microsoft.com/source/)ä¸Šã§ç¨¼åƒã—ãŸã¨ã“ã‚ã€æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã€**8.53%ã®ã‚¯ãƒªãƒƒã‚¯ç‡ã®å‘ä¸ŠãŒè¦‹ã‚‰ã‚ŒãŸ**ã¨ã®ã“ã¨ã§ã™ã€‚

å½¼ã‚‰ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¤œè¨¼ã®ä¸­ã§æœ€ã‚‚é«˜ã„æ€§èƒ½ãŒå¾—ã‚‰ã‚ŒãŸã®ãŒNRMS[9]ã¨ã„ã†ãƒ¢ãƒ‡ãƒ«ã«BERTã‚’é©ç”¨ã—ãŸ**NRMS-BERT**ã¨ã„ã†ã€BERTã‹ã‚‰ã®å‡ºåŠ›ç³»é•·ã«Multihead Attentionã‚’é©ç”¨ã—ãŸæ‰‹æ³•ã§ã™ã€‚æ¬¡ç« ã§ã¯ã€ã„ã‚ˆã„ã‚ˆä»Šå›å®Ÿè£…ã—ãŸNRMS-BERTã®è©³ç´°ãªç†è«–ãŠã‚ˆã³å®Ÿè£…ã®èª¬æ˜ã‚’ã—ã¦ã„ãã¾ã™ã€‚


# 3. NRMS-BERTã®ç†è«–ã¨å®Ÿè£…

## 3.1 ä½¿ç”¨æŠ€è¡“ãƒ»ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ

ã¾ãšã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¦‚è¦³ã™ã‚‹ãŸã‚ã«ã€ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```bash
$ tree -L 2
â”œâ”€â”€ README.md
â”œâ”€â”€ dataset/ # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã‚³ãƒ¼ãƒ‰, ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ ¼ç´ã•ã‚Œã‚‹
â”‚   â””â”€â”€ download_mind.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements-dev.lock # Ryeã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ requirements.lock # Ryeã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/ # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç­‰ãŒå®šç¾©ã•ã‚ŒãŸconfig.pyãŒå­˜åœ¨
â”‚   â”œâ”€â”€ const/ # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…±é€šã®å®šæ•°
â”‚   â”œâ”€â”€ evaluation/ # MRRã‚„nDCGç­‰ã®è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
â”‚   â”œâ”€â”€ experiment/	# å®Ÿé¨“ç”¨ã‚³ãƒ¼ãƒ‰(train.py)
â”‚   â”œâ”€â”€ mind/ # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–¢é€£ã®ã‚³ãƒ¼ãƒ‰ (PyTorchã®Datasetã‚¯ãƒ©ã‚¹ç­‰)
â”‚   â”œâ”€â”€ recommendation/ # æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
â”‚   â”‚   â””â”€â”€ nrms/ # NRMS-BERTã®å®Ÿè£…
â”‚   â”‚       â”œâ”€â”€ AdditiveAttention.py
â”‚   â”‚       â”œâ”€â”€ NRMS.py
â”‚   â”‚       â”œâ”€â”€ PLMBasedNewsEncoder.py
â”‚   â”‚       â”œâ”€â”€ UserEncoder.py
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ utils/ # æ±ç”¨é–¢æ•°
â””â”€â”€ test/ # ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
    â”œâ”€â”€ evaluation/
    â”œâ”€â”€ mind/
    â””â”€â”€ recommendation/
```

ä½¿ç”¨ã—ãŸæŠ€è¡“ã¯ä»¥ä¸‹ã«ãªã‚Šã¾ã™ã€‚

- è¨€èª: Python
- æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: PyTorch, huggingface/transformers
- ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†ãƒ»ä»®æƒ³ç’°å¢ƒ: Rye
- Linterã‚„Formatterç­‰ã®é–‹ç™ºåŸºç›¤å‘¨ã‚Š: Ruff, mypy, black

ä»Šå›ã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†ãƒ»ä»®æƒ³ç’°å¢ƒã®ä½œæˆã ã‘ã€å°‘ã—æ€ã„åˆ‡ã£ã¦[Rye](https://rye-up.com/)ã‚’æ¡ç”¨ã—ã¾ã—ãŸã€‚

Ryeã¯Flaskã®ä½œè€…ã§ã‚ã‚‹[Armin Ronacher](https://github.com/mitsuhiko)ã«ã‚ˆã‚Šé–‹ç™ºã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ãƒªãƒã‚¸ãƒˆãƒªã®Disscussionã®[Should Rye Exist?](https://github.com/mitsuhiko/rye/discussions/6)ã«è¨˜è¼‰ãŒã‚ã‚‹é€šã‚Šã€Rustã«ãŠã‘ã‚‹cargoã‚„rustupã®ã‚ˆã†ã«ã€**Pythonã«ãŠã‘ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã«ãªã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™**ã€‚

Ryeã¯**one-stop-shop**ã¨è¡¨ç¾ã•ã‚Œã‚‹ã‚ˆã†ã«ã€Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã®ç®¡ç†ã‚„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã„ã£ãŸPythoné–‹ç™ºã«å¿…è¦ãªç®¡ç†ã‚’ä¸€é€šã‚Šæ‹…ã£ã¦ãã‚Œã¾ã™ã€‚poetry + pyenv,asdfã®ã‚ˆã†ã«ã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†ã¨ã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ç®¡ç†ã§ãã‚Œãã‚Œãƒ„ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚Ryeã•ãˆå…¥ã‚Œã¦ãŠã‘ã°ã€`rye sync`ã¨ã‚³ãƒãƒ³ãƒ‰ã‚’ä¸€ã¤æ‰“ã¤ã ã‘ã§ã€`pyproject.yaml`ã«å‰‡ã£ãŸPythonå®Ÿè¡Œç’°å¢ƒãŒä½œæˆã•ã‚Œã¾ã™ã€‚

ç¾æ™‚ç‚¹ã§ã¯ã‚ãã¾ã§ã€**experimentalã§not yet production ready**ã¨[è¨˜è¼‰ãŒã‚ã‚‹](https://rye-up.com/)ã®ã§ã€æœ¬ç•ªåˆ©ç”¨ç­‰ã‚’å¼·ãå‹§ã‚ã‚‹ã“ã¨ã¯ã€èºŠèº‡ã‚ã‚Œã¾ã™ã€‚ã—ã‹ã—ã€å€‹äººã®æ„Ÿæƒ³ã¨ã—ã¦ã¯ã€æ‚ªåé«˜ãPythoné–‹ç™ºç’°å¢ƒæ§‹ç¯‰ã‚’DXã—ã¦ãã‚Œã‚‹ç´ æ™´ã‚‰ã—ã„ãƒ„ãƒ¼ãƒ«ã§ã‚ã‚‹ã¨æ„Ÿã˜ã¦ã„ã¾ã™ã€‚


## 3.2 PyTorchã«ã‚ˆã‚‹PLM-NR(NRMS-BERT)å®Ÿè£…

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¦‚è¦³ãŒã¤ã‹ã‚ãŸã¨ã“ã‚ã§ã€NRMS-BERTã¨ãã‚Œã‚’æ§‹æˆã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®PyTorch/transformersã«ã‚ˆã‚‹å®Ÿè£…ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚ãªãŠã€Multihead Attentionã®å®Ÿè£…ãƒ»æ•°å¼ã«é–¢ã™ã‚‹èª¬æ˜ã¯ã“ã“ã§ã¯å‰²æ„›ã—ã¾ã™ã€‚[Transformerã®åŸè«–æ–‡](https://arxiv.org/abs/1706.03762)ã‚„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã®æŠ€è¡“è¨˜äº‹ç­‰ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### Additive Attention

NewsEncoderãƒ»UserEncoderã«ã¤ã„ã¦ã¿ã¦è¡Œãå‰ã«ã€ã¾ãšã¯Additive Attentionã«ã¤ã„ã¦ã§ã™ã€‚Additive Attentionã¯ã€Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®é…åˆ—ã€ã‚„ã€Œãƒ¦ãƒ¼ã‚¶ãŒéå»ã«èª­ã‚“ã ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®é…åˆ—ã€ãªã©ã®**ãƒ™ã‚¯ãƒˆãƒ«ç³»åˆ—ã‚’ã€é‡è¦åº¦ã«åŸºã¥ã„ã¦é‡ã¿ä»˜ã‘ã—ã€ä¸€ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã«é›†ç´„ã™ã‚‹å½¹å‰²ã‚’æŒã¡ã¾ã™**ã€‚

ä»¥ä¸‹ãŒæ•°å¼ã§ã™: 

$$ a_i^w = q_w^T {\tanh}(V_w \times h_i^w + v_w) $$

$$ {\alpha}_i^w = \frac{\exp{(a_i^w)}}{\sum_{i=1}^{M}\exp{(a_i^w)}} $$

$$ r = \sum_{i=1}^{M} {\alpha}_i^w h_i^w $$

Additive Attentionã§ã¯ã€$d$æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’$M$å€‹ä¸¦ã¹ãŸ$M \times d$æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ç³»åˆ—$h$(e.g. $d$æ¬¡å…ƒã®å˜èªãƒ™ã‚¯ãƒˆãƒ«ã‚’$M$å˜èªä¸¦ã¹ãŸæ–‡ç« )ã‚’2ã¤ã®ç·šå½¢å¤‰æ›å±¤ã¨æ­£è¦åŒ–ã«ã‚ˆã‚Šã€$M$å€‹ã‚ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã®ãã‚Œãã‚Œã®é‡è¦åº¦(e.g. æ–‡ç« ã«ãŠã‘ã‚‹å˜èªã®é‡è¦åº¦ )ã‚’è¡¨ç¾ã—ãŸé‡ã¿ãƒ™ã‚¯ãƒˆãƒ«$\alpha$($M \times 1$)ã‚’ç²å¾—ã—ã¾ã™ã€‚

ãã—ã¦ã€é‡ã¿$\alpha$ã§$M \times d$æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ç³»åˆ—$h$ã‚’åŠ é‡å¹³å‡ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€$M$å€‹ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€é‡è¦åº¦ã«åŸºã¥ã„ãŸé‡ã¿ä»˜ã‘ãŒãªã•ã‚ŒãŸä¸€ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«$r$ã«é›†ç´„ã—ã¾ã™ã€‚

ãã‚Œã§ã¯ã€Additive Attentionã®è¨ˆç®—ãŒã‚ã‹ã£ãŸã¨ã“ã‚ã§ã€PyTorchã«ã‚ˆã‚‹å®Ÿè£…ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
import torch
from torch import nn


def init_weights(m: nn.Module) -> None:
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight.data)
        if m.bias is not None:
            nn.init.zeros_(m.bias)


class AdditiveAttention(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int) -> None:
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(
                input_dim, hidden_dim
            ),  # in: (batch_size, M, d), out: (batch_size, M, hidden_dim)
            nn.Tanh(),  # in: (batch_size, M, hidden_dim), out: (batch_size, seq_len, hidden_dim)
            nn.Linear(
                hidden_dim, 1, bias=False
            ),  # in: (batch_size, M, hidden_dim), out: (batch_size, M, 1)
            nn.Softmax(dim=-2),
        )
        self.attention.apply(init_weights)

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        attention_weight = self.attention(input) # = Î± 
        return input * attention_weight
```

ä»¥ä¸ŠãŒAdditive Attentionã®èª¬æ˜ã«ãªã‚Šã¾ã™ã€‚NRMS-BERTã§ã¯ã€

- NewsEncoderã«ãŠã‘ã‚‹å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®ç³»åˆ—ã®é›†ç´„
- UserEncoderã«ãŠã‘ã‚‹ãƒ¦ãƒ¼ã‚¶ãŒéå»ã«èª­ã‚“ã ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ™ã‚¯ãƒˆãƒ«ç³»åˆ—ã®é›†ç´„

ã®2ã¤ã«Additive AttentionãŒç”¨ã„ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

### News Encoder

ã“ã“ã§ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆã®å˜èªåˆ—ã‚’$d$æ¬¡å…ƒã®å˜ä¸€ã®ãƒ™ã‚¯ãƒˆãƒ«$r$ã«å¤‰æ›ã™ã‚‹News Encoderã®å®Ÿè£…ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚ä»¥ä¸‹ã¯ã€NRMS-BERTã«ãŠã‘ã‚‹News Encoderã®æ¦‚è¦å›³ã§ã™ã€‚

![](/images/d028faf7217ec8/news-encoder.png =400x)

NRMS-BERTã®News Encoderã¯ã€BERT Encoderãƒ»Multihead Attentionãƒ»Additive Attentionã®3ã¤ã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚

News Encoderã§ã¯ã€ã¾ãšã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ãƒ†ã‚­ã‚¹ãƒˆ$D$ã®å˜èªç³»åˆ—$w_1$ ~ $w_M$ã‚’BERTã«å…¥åŠ›ã—ã¾ã™ã€‚ãã†ã—ã¦å¾—ã‚‰ã‚ŒãŸå˜èªãƒ™ã‚¯ãƒˆãƒ«ã®åŸ‹ã‚è¾¼ã¿$e_1$ ~ $e_M$ã‚’Multihead Self-Attention, Additive Attentionã®é †ç•ªã§å…¥åŠ›ã—ã€æœ€çµ‚çš„ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ™ã‚¯ãƒˆãƒ«$h$ã‚’ç²å¾—ã—ã¾ã™ã€‚

ä»Šå›ã€BERT Encoderã¨Multihead Attentionã¯ã€transformersã¨PyTorchã«ã‚ˆã‚Šæä¾›ã•ã‚Œã¦ã„ã‚‹å®Ÿè£…ã‚’æ´»ç”¨ã—ã¾ã—ãŸã€‚Additive Attentionã‚‚æ—¢ã«å®Ÿè£…æ¸ˆã¿ã§ã‚ã‚‹ãŸã‚ã€News Encoderè‡ªä½“ã®å®Ÿè£…ã¯éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ã€‚

ãã‚Œã§ã¯ã€ä»¥ä¸Šã‚’è¸ã¾ãˆã€News Encoder(`PLMBasedNewsEncoder`)ã®å®Ÿè£…ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
import torch
from torch import nn
from transformers import AutoConfig, AutoModel

from .AdditiveAttention import AdditiveAttention


class PLMBasedNewsEncoder(nn.Module):
    def __init__(
        self,
        pretrained: str = "bert-base-uncased",
        multihead_attn_num_heads: int = 16,
        additive_attn_hidden_dim: int = 200,
    ):
        super().__init__()
        self.plm = AutoModel.from_pretrained(pretrained)

        plm_hidden_size = AutoConfig.from_pretrained(pretrained).hidden_size

        self.multihead_attention = nn.MultiheadAttention(
            embed_dim=plm_hidden_size, num_heads=multihead_attn_num_heads, batch_first=True
        )
        self.additive_attention = AdditiveAttention(plm_hidden_size, additive_attn_hidden_dim)

    def forward(self, input_val: torch.Tensor) -> torch.Tensor:
        V = self.plm(input_val).last_hidden_state  # [batch_size, M] -> [batch_size, seq_len, d]
        multihead_attn_output, _ = self.multihead_attention(
            V, V, V
        )  # [batch_size, M, d] -> [batch_size, M, d]
        additive_attn_output = self.additive_attention(
            multihead_attn_output
        )  # [batch_size, M, d] -> [batch_size, M, d]
        output = torch.sum(
            additive_attn_output, dim=1
        )  # [batch_size, M, d] -> [batch_size, d]

        return output
```

### User Encoder

ã“ã“ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒéå»ã«èª­ã‚“ã ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆã‹ã‚‰ã€News Encoderã®å‡ºåŠ›æ¬¡å…ƒã¨åŒã˜$d$æ¬¡å…ƒã®ãƒ¦ãƒ¼ã‚¶ã®ãƒ™ã‚¯ãƒˆãƒ«$u$ã‚’ç²å¾—ã™ã‚‹User Encoderã®å®Ÿè£…ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚ä»¥ä¸‹ã¯ã€NRMS-BERTã«ãŠã‘ã‚‹User Encoderã®æ¦‚è¦å›³ã§ã™ã€‚

![](/images/d028faf7217ec8/user-encoder.png =500x)

User Encoderã§ã¯ã€ã¾ãšã€æ—¢ã«å®Ÿè£…ã—ãŸNews Encoderã‚’ç”¨ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãŒéå»ã«èª­ã‚“ã $T$æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆ$D_1$ ~ $D_T$(List of Clicked News Content)ã‚’ã€ãã‚Œãã‚Œ$d$æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«$h_1$ ~ $h_T$ã«å¤‰æ›ã—ã€$T$å€‹ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç²å¾—ã—ã¾ã™ã€‚

$h_1$ ~ $h_T$ã‚’ã€News EncoderãŒå˜èªãƒ™ã‚¯ãƒˆãƒ«åˆ—ã‚’ä¸€ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã«é›†ç´„ã—ãŸã®ã¨åŒæ§˜ã®æ‰‹é †ã§ã€Multihead Attention,Additive Attentionã®é †ã§é©ç”¨ã—ã¦ã€æœ€çµ‚çš„ã«ã€ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’è¡¨ç¾ã—ãŸä¸€ã¤ã®$d$æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«$u$ã«é›†ç´„ã—ã¾ã™ã€‚

ä»¥ä¸ŠãŒUser Encoderã®æ§‹é€ ã«é–¢ã™ã‚‹èª¬æ˜ã§ã™ã€‚ã“ã¡ã‚‰ã‚‚News Encoder, Additive Attention, Multihead Attentionã¨ã„ã£ãŸæ§‹æˆè¦ç´ ã¯æ—¢ã«å®Ÿè£…æ¸ˆã¿ã§ã‚ã‚‹ãŸã‚ã€å®Ÿè£…ã¯éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ã€‚User Encoder(`UserEncoder`)ã®å®Ÿè£…ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
import torch
from torch import nn

from .AdditiveAttention import AdditiveAttention


class UserEncoder(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        multihead_attn_num_heads: int = 16,
        additive_attn_hidden_dim: int = 200,
    ) -> None:
        super().__init__()
        self.hidden_size = hidden_size
        self.multihead_attention = nn.MultiheadAttention(
            embed_dim=hidden_size, num_heads=multihead_attn_num_heads, batch_first=True
        )
        self.additive_attention = AdditiveAttention(hidden_size, additive_attn_hidden_dim)

    def forward(self, news_histories: torch.Tensor, news_encoder: nn.Module) -> torch.Tensor:
        batch_size, hist_size, seq_len = news_histories.size()
        news_histories = news_histories.view(
            batch_size * hist_size, seq_len
        )  # [batch_size, N, M] -> [batch_size*N, M]

        news_histories_encoded = news_encoder(
            news_histories
        )  # [batch_size*N, M] -> [batch_size*N, d]

        news_histories_encoded = news_histories_encoded.view(
            batch_size, hist_size, self.hidden_size
        )  # [batch_size*N, d] -> [batch_size, N, d]

        multihead_attn_output, _ = self.multihead_attention(
            news_histories_encoded, news_histories_encoded, news_histories_encoded
        )  # [batch_size, N, d] -> [batch_size, N, d]

        additive_attn_output = self.additive_attention(
            multihead_attn_output
        )  # [batch_size, N, d] -> [batch_size, d]

        output = torch.sum(additive_attn_output, dim=1) 

        return output
```


### NRMS-BERT(Click Predictor)

æœ€å¾Œã«NRMS-BERT(Click Predictor)ã®å®Ÿè£…ã‚’ã¿ã¦ã„ãã¾ã™ã€‚ä»Šã¾ã§ã§å®Ÿè£…ã—ãŸUser Encoderã¨News Encoderã‚’ä½¿ã„ã€æ¨è–¦å€™è£œã¨ãªã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚„ãƒ¦ãƒ¼ã‚¶ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–($h$,$u$)ã—ã€ãã®å†…ç©$u^T \cdot h$ã‚’ç®—å‡ºã—ã€ãƒ¦ãƒ¼ã‚¶ãŒhã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ç¢ºç‡ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚ä»¥ä¸‹ãŒãã®å®Ÿè£…ã§ã™ã€‚

```python
import torch
from torch import nn
from transformers.modeling_outputs import ModelOutput


class NRMS(nn.Module):
    def __init__(
        self,
        news_encoder: nn.Module,
        user_encoder: nn.Module,
        hidden_size: int,
        loss_fn: nn.Module = nn.CrossEntropyLoss(),
    ) -> None:
        super().__init__()
        self.news_encoder: nn.Module = news_encoder
        self.user_encoder: nn.Module = user_encoder
        self.hidden_size: int = hidden_size
        self.loss_fn = loss_fn

    def forward(
        self, candidate_news: torch.Tensor, news_histories: torch.Tensor, target: torch.Tensor
    ) -> torch.Tensor:
        """
        Parameters
        ----------
        candidate_news : torch.Tensor (shape = (batch_size, candidate_num, seq_len))
        news_histories : torch.Tensor (shape = (batch_size, candidate_num, seq_len))
        ===========================================================================

        Returns
        ----------
        output: torch.Tensor (shape = (batch_size, candidate_num))
        """

        batch_size, candidate_num, seq_len = candidate_news.size()
        candidate_news = candidate_news.view(batch_size * candidate_num, seq_len)
        news_candidate_encoded = self.news_encoder(
            candidate_news
        )  # [batch_size * (candidate_num), seq_len] -> [batch_size * (candidate_num), emb_dim]
        news_candidate_encoded = news_candidate_encoded.view(
            batch_size, candidate_num, self.hidden_size
        )  # [batch_size * (candidate_num), emb_dim] -> [batch_size, (candidate_num), emb_dim]

        news_histories_encoded = self.user_encoder(
            news_histories, self.news_encoder
        )  # [batch_size, histories, seq_len] -> [batch_size, emb_dim]
        news_histories_encoded = news_histories_encoded.unsqueeze(
            -1
        )  # [batch_size, emb_dim] -> [batch_size, emb_dim, 1]

        output = torch.bmm(
            news_candidate_encoded, news_histories_encoded
        )  # [batch_size, (candidate_num), emb_dim] x [batch_size, emb_dim, 1] -> [batch_size, (1+npratio), 1, 1]
        output = output.squeeze(-1).squeeze(-1)  # [batch_size, (1+npratio), 1, 1] -> [batch_size, (1+npratio)]

        # NOTE:
        # when "val" mode(self.training == False) â†’ not calculate loss score
        # Multiple hot labels may exist on target.
        # e.g.
        # candidate_news = ["N24510","N39237","N9721"]
        # target = [0,2](=[1, 0, 1] in one-hot format)
        if not self.training:
            return ModelOutput(logits=output, loss=torch.Tensor([-1]), labels=target)

        loss = self.loss_fn(output, target)
        return ModelOutput(logits=output, loss=loss, labels=target)
```


ä»¥ä¸ŠãŒãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ã«é–¢ã™ã‚‹èª¬æ˜ã«ãªã‚Šã¾ã™ã€‚æ¬¡ç« ã‹ã‚‰ã¯ã€å®Ÿè£…ã—ãŸãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ»æ¤œè¨¼ã‚’è¡Œã„ã¾ã™ã€‚ä»Šå›ã€ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ã«ã¯**MIND**ã¨ã„ã†ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¾ã—ãŸã€‚



# 4. MIND: Microsoft News Dataset

ä»Šå›ã€**ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»è©•ä¾¡ã«ã¯Microsoft Newsã®å®Ÿéš›ã®è¡Œå‹•ãƒ­ã‚°ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šä½œæˆã•ã‚ŒãŸ Microsoft News Dataset**(**é€šç§°:MIND**)[3]ã‚’ç”¨ã„ã¾ã—ãŸã€‚MINDã«ã¯ã€Œç´„16ä¸‡ä»¶ã®è‹±æ–‡ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã€ã¨ã€Œç´„100ä¸‡ã®ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰åé›†ã•ã‚ŒãŸ1500ä¸‡ä»¶ä»¥ä¸Šã®è¡Œå‹•ãƒ­ã‚°ã€ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚2020å¹´ã«Microsoftã®ç ”ç©¶è€…ã‚‰ã«ã‚ˆã£ã¦å…¬é–‹ã•ã‚Œã¦ä»¥æ¥ã€MINDã¯å¤šãã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«é–¢ã™ã‚‹ç ”ç©¶ã§ç”¨ã„ã‚‰ã‚Œã¦ã„ã¾ã™
	
ä»Šå›ã¯ã€MINDå†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‚’æ ¼ç´ã—ãŸ`news.tsv`ã¨ãƒ¦ãƒ¼ã‚¶ã®Impressionæƒ…å ±ã‚’æ ¼ç´ã—ãŸ`behaviors.tsv`ã®äºŒã¤ã®tsvãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»æ¤œè¨¼ã‚’è¡Œã„ã¾ã—ãŸã€‚[Microsoftã®å…¬å¼ã‚µã‚¤ãƒˆ](https://learn.microsoft.com/en-us/azure/open-datasets/dataset-microsoft-news)ã«ã‚ˆã‚‹ã¨ã€ãã‚Œãã‚Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯æ¬¡ã®è¡¨ã«ç¤ºã™ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿ãŒæ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ã€‚


**news.tsv**
| ã‚«ãƒ©ãƒ å             | èª¬æ˜                                                                                           | å…·ä½“ä¾‹                                                                                      |
|--------------------|----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| News ID            | ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ID                                                                                     | N37378                                                                                     |
| Category           | ã‚«ãƒ†ã‚´ãƒª                                                                                        | sports                                                                                    |
| Subcategory        | ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª                                                                                     | golf                                                                                      |
| Title              | ã‚¿ã‚¤ãƒˆãƒ«                                                                                        | PGA Tour winners                                                                          |
| Abstract           | è¦ç´„                                                                                            | A gallery of recent winners on the PGA Tour.                                               |
| URL                | URL                                                                                           | [https://www.msn.com/en-us/sports/golf/pga-tour-winners/ss-AAjnQjj?ocid=chopendata](https://www.msn.com/en-us/sports/golf/pga-tour-winners/ss-AAjnQjj?ocid=chopendata) |
| Title Entities     | ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£                                                       | -                                                                                         |
| Abstract Entities  | ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¦ç´„ã«å«ã¾ã‚Œã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£                                                           | -                                                                                         |

**behavior.tsv:**
| ã‚«ãƒ©ãƒ å       | èª¬æ˜                                                                                       | å…·ä½“ä¾‹                   |
|--------------|------------------------------------------------------------------------------------------|------------------------|
| Impression ID | ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ID                                                                         | 123                     |
| User ID      | ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åŒ¿åID                                                                             | U131                    |
| Time         | ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®æ™‚é–“ã€‚å½¢å¼ã¯â€œMM/DD/YYYY HH:MM:SS AM/PMâ€                                      | 11/13/2019 8:36:57 AM   |
| History      | ã“ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®å‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒªãƒƒã‚¯å±¥æ­´ï¼ˆã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®IDãƒªã‚¹ãƒˆï¼‰       | N11 N21 N103            |
| Impressions  | ã“ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã§è¡¨ç¤ºã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆã¨ã€ãã‚Œã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«å¯¾ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ãƒªãƒƒã‚¯è¡Œå‹•ï¼ˆ1ã¯ã‚¯ãƒªãƒƒã‚¯ã€0ã¯éã‚¯ãƒªãƒƒã‚¯ï¼‰ | N4-1 N34-1 N156-0 N207-0 N198-0 |

Historyã‚«ãƒ©ãƒ ã«ç¤ºã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’ãƒ¦ãƒ¼ã‚¶ãŒéå»ã«ã‚¯ãƒªãƒƒã‚¯ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹($D_1$ ~ $D_T$)ã€Impressionsã‚«ãƒ©ãƒ ã«ç¤ºã•ã‚ŒãŸã‚¯ãƒªãƒƒã‚¯, éã‚¯ãƒªãƒƒã‚¯æƒ…å ±ã‚’æ­£è§£ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ã„ã¾ã™ã€‚

MINDã«é–¢ã™ã‚‹ã‚ˆã‚Šè©³ç´°ãªèª¬æ˜ã¯ã€[è«–æ–‡](https://aclanthology.org/2020.acl-main.331/)ã‚„[å…¬å¼ã‚µã‚¤ãƒˆ](https://msnews.github.io/)ã‚’ã”è¦§ãã ã•ã„ã€‚ãªãŠã€MINDã«ã¯ãƒ¦ãƒ¼ã‚¶æ•°ã‚’50,000äººåˆ†ã®ã¿ã«é™å®šã—ãŸsmallã‚µã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ( MIND-small )ã‚‚ç”¨æ„ã•ã‚Œã¦ãŠã‚Šã€ä»Šå›ã®å®Ÿé¨“ã§ã¯ãã¡ã‚‰ã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚

# 5. ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡

## 5.1 ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

NRMS-BERTã¯ã€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´æ–¹æ³•ãŒå°‘ã—ç‰¹æ®Šã§ã™ã€‚MINDã«ã‚ã‚‹ã‚ˆã†ãªã‚¯ãƒªãƒƒã‚¯ãƒ­ã‚°ã‹ã‚‰ã€å˜ç´”ã«ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹(=1), ã‚¯ãƒªãƒƒã‚¯ã—ãªã„(=0)ã®äºŒå€¤åˆ†é¡ã¨ã—ã¦å­¦ç¿’ã™ã‚‹ã®ã§ã¯ãªãã€**ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**ã¨ã„ã†æ‰‹æ³•ã‚’æ´»ç”¨ã—ã¾ã™ã€‚ 

ãƒ¦ãƒ¼ã‚¶ãŒã‚¯ãƒªãƒƒã‚¯ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹1æœ¬(æ­£ä¾‹,label=1)ã«å¯¾ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãŒã‚¯ãƒªãƒƒã‚¯ã—ãªã‹ã£ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹Kæœ¬(è² ä¾‹,label=0)ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€(1 + K)ã‚¯ãƒ©ã‚¹ã®å¤šå€¤åˆ†é¡ã¨ã—ã¦å­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚

ã“ã“ã§ã¯ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹PyTorchã®Datasetã‚¯ãƒ©ã‚¹ã®å®Ÿè£…ã‚’å‚è€ƒã«è¦‹ã¦ã„ãã¾ã™

```python
...

class MINDTrainDataset(Dataset):
    def __init__(
        ...
    ) -> None:
        self.behavior_df: pl.DataFrame = behavior_df # behavior.tsvãŒæ ¼ç´ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        self.news_df: pl.DataFrame = news_df # news.tsvãŒæ ¼ç´ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        self.npratio: int = npratio # ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã®è² ä¾‹ã®æ•°K
	...

    def __getitem__(self, behavior_idx: int) -> dict:  # TODO: ä¸€è¡Œã‚ãŸã‚Šã«positiveãŒè¤‡æ•°å­˜åœ¨ã™ã‚‹ã“ã¨ã‚‚è€ƒæ…®ã—ãŸ
	...
    	# Extract Values
        behavior_item = self.behavior_df[behavior_idx]    
	...

	# Sampling Positive(clicked) & Negative(non-clicked) Sample
        poss_idxes, neg_idxes = (
            behavior_item["clicked_idxes"].to_list()[0],
            behavior_item["non_clicked_idxes"].to_list()[0],
        )
        sample_poss_idxes, sample_neg_idxes = random.sample(poss_idxes, 1), self.__sampling_negative(
            neg_idxes, self.npratio
        )
        sample_impression_idxes = sample_poss_idxes + sample_neg_idxes
        random.shuffle(sample_impression_idxes)
        sample_impressions = impressions[sample_impression_idxes]
	...
    def __sampling_negative(self, neg_idxes: list[int], npratio: int) -> list[int]:
        if len(neg_idxes) < npratio:
            return neg_idxes + [EMPTY_IMPRESSION_IDX] * (npratio - len(neg_idxes))

        return random.sample(neg_idxes, self.npratio)
	...
```
ãªãŠã€ä»Šå›ã¯å…ˆè¡Œç ”ç©¶[1,9]ã¨åŒæ§˜ã€è² ä¾‹ã®æ•°ã‚’K = 4(`npratio = 4`)ã€ã™ãªã‚ã¡ã€Œ5æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ä¸­ã§ãƒ¦ãƒ¼ã‚¶ãŒã‚¯ãƒªãƒƒã‚¯ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã©ã‚Œã‹ï¼Ÿã€ã¨ã„ã†5å€¤åˆ†é¡ã¨ã—ã¦æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’è¡Œã„ã¾ã—ãŸã€‚


## 5.2 è©•ä¾¡æŒ‡æ¨™

ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™ã¯å…ƒè«–æ–‡[1,9]ã«å€£ã„ã€AUC, MRR, nDCG@Kã®3ã¤ã‚’æ¡ç”¨ã—ã¾ã—ãŸã€‚AUCã¯2å€¤åˆ†é¡ã§ã‚ˆãç”¨ã„ã‚‰ã‚Œã‚‹è©•ä¾¡æŒ‡æ¨™ã®ä¸€ã¤ã§ã€ã‚¯ãƒªãƒƒã‚¯ã—ãŸãƒ»ã—ãªã‹ã£ãŸã‚’é©åˆ‡ã«äºˆæ¸¬ã§ãã¦ã„ã‚‹æ™‚ã»ã©ã‚¹ã‚³ã‚¢ãŒé«˜ããªã‚Šã¾ã™ã€‚

MRRã¯ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ã®è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ã‚ˆãç”¨ã„ã‚‰ã‚Œã€ä»¥ä¸‹ã®å¼ã§è¡¨ã›ã¾ã™ã€‚

$$
	MRR = \frac{1}{U} \sum_{u=1}^U \frac{1}{k_u}
$$
		
MRRã¯ã€äºˆæ¸¬ã—ãŸé †ä½ã®ä¸­ã®æœ€åˆã®é©åˆã‚¢ã‚¤ãƒ†ãƒ ã®é †ä½ã€ã™ãªã‚ã¡ã€äºˆæ¸¬ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒƒã‚¯ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ä¸­ã§æœ€ã‚‚æœ€åˆã«ç¾ã‚Œã‚‹å®Ÿéš›ã«ã‚¯ãƒªãƒƒã‚¯ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é †ä½ã®é€†æ•°ã®å…¨ãƒ¦ãƒ¼ã‚¶ã«å¯¾ã™ã‚‹å¹³å‡å€¤ã§ã™ã€‚

nDCGã‚‚ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ã®è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ãƒãƒ”ãƒ¥ãƒ©ãƒ¼ã§ä»¥ä¸‹ã®å¼ã§è¡¨ã›ã¾ã™ã€‚

$$
	DCG = \sum_{i=1}^{K}\frac{(2^{r_i} - 1)}{\log{(1+i)}}
$$

$$
	nDCG = \frac{DCG}{DCG_{max}}
$$

ãªãŠã€å…ƒè«–æ–‡ã«åˆã‚ã›ã¦ã€Burgesã‚‰ã«ã‚ˆã‚Šå®šç¾©ã•ã‚ŒãŸnDCG[8]ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

ã“ã‚Œã‚‰ã®è©•ä¾¡æŒ‡æ¨™ã¯[RecEvaluator](https://github.com/YadaYuki/news-recommendation-llm/blob/main/src/evaluation/RecEvaluator.py#L13)ã‚¯ãƒ©ã‚¹ã«å®Ÿè£…ãŒã‚ã‚Šã¾ã™ã€‚é•·ããªã‚‹ã®ã§ã“ã“ã§ã¯å‰²æ„›ã—ã¾ã™ãŒã€èˆˆå‘³ãŒã‚ã‚Œã°ã€ãœã²ã”è¦§ãã ã•ã„ã€‚

## 5.3 è©•ä¾¡çµæœ

ã•ã¦ã€ä»¥ä¸Šã‚’è¸ã¾ãˆã¦ã€ã„ã‚ˆã„ã‚ˆè©•ä¾¡çµæœã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã“ã‚Œã¾ã§ã§ç´¹ä»‹ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’NVIDIA V100 GPU x 1ä¸Šã§ã€MIND-smallã‚’ç”¨ã„ã¦å­¦ç¿’ã—ã¾ã—ãŸã€‚ã¾ãŸæ€§èƒ½ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦ã‚‚å®Ÿè£…ãƒ»è©•ä¾¡ã‚’è¡Œã„ã¾ã—ãŸã€‚

å®Ÿé¨“çµæœã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚

|         Model          |  AUC  |  MRR  | nDCG@5 | nDCG@10 | 
| :--------------------: | :----:| :----:| :----: | :-----: |
| Random Recommendation  | 0.500 | 0.201 | 0.203  |  0.267  |
| NRMS + DistilBERT-base | 0.674 | 0.297 | 0.322  |  0.387  |
|    NRMS + BERT-base    | 0.689 | 0.306 | 0.336  |  0.400  |
|    NRMS-BERT[1] (å‚è€ƒ)   | 0.695 | 0.347 | 0.380  |  0.437  |


ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€DistilBERT, BERTã€ã„ãšã‚Œã‚‚æ˜ã‚‰ã‹ã«é«˜ã„æ€§èƒ½ãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

ã¾ãŸã€çµæœã®è¡¨ã®æœ€ä¸‹éƒ¨ã«ã€PLM-NRã®è«–æ–‡ã«è¨˜è¼‰ã•ã‚Œã¦ã„ãŸNRMS-BERTã®çµæœã‚’å‚è€ƒã¨ã—ã¦æ²è¼‰ã—ã¾ã—ãŸã€‚ä»Šå›ã¯ã€å°è¦æ¨¡ãªMIND-smallã§å­¦ç¿’ã—ã¾ã—ãŸãŒã€ãã‚Œã§ã‚‚ã€**MINDå…¨ä½“ã§å­¦ç¿’ã—ãŸè«–æ–‡è¨˜è¼‰ã®çµæœã«ã‹ãªã‚Šè¿‘ã„æ€§èƒ½ã‚’å‡ºã›ã¦ã„ã‚‹ã“ã¨**ãŒã‚ã‹ã‚Šã¾ã™ã€‚

ãªãŠã€å­¦ç¿’ã«ã‹ã‹ã£ãŸæ™‚é–“ã‚„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¸ã®ãƒªãƒ³ã‚¯ã¯ã€Appendixã«æ²è¼‰ã—ã¾ã—ãŸã®ã§ã€ãã¡ã‚‰ã‚’ã”è¦§ãã ã•ã„ã€‚

# 6. ã¾ã¨ã‚

é•·ããªã‚Šã¾ã—ãŸãŒã€ä»¥ä¸Šã«ãªã‚Šã¾ã™ï¼

ä»Šå›ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(BERT)ã‚’ç”¨ã„ãŸæ¨è–¦æ‰‹æ³•ã§ã‚ã‚‹NRMS-BERTã®å®Ÿè£…ãƒ»è©•ä¾¡ã‚’è¡Œã„ã¾ã—ãŸã€‚MIND(MIND-small)ã¨ã„ã†ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦è¨“ç·´ãƒ»è©•ä¾¡ã‚’è¡Œã£ãŸçµæœã€NRMS-BERTã®ç¾è«–æ–‡ã«ã‹ãªã‚Šè¿«ã‚‹æ€§èƒ½ã‚’å‡ºã™ã“ã¨ãŒã§ãã¾ã—ãŸã€‚

ä»Šå›ã¯ã€BERT-base/DistilBERT-baseã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã—ãŸãŒã€2023å¹´ã«å…¥ã£ã¦ç™ºè¡¨ã•ã‚ŒãŸç›´è¿‘ã®ç ”ç©¶ã§ã¯ã€GPTç³»ãƒ¢ãƒ‡ãƒ« + LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç”¨ã„ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®æ‹¡å¼µ[7]ã‚„ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®ãŸã‚ã®Prompt Learning[6]ç­‰ã€æ–°ã—ã„æ‰‹æ³•ãŒæ¬¡ã€…ã¨ç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å¿œç”¨åˆ†é‡ã®ä¸€ã¤ã¨ã—ã¦ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦åˆ†é‡ã®ç ”ç©¶ã¯ã€ã¾ã™ã¾ã™æ´»ç™ºåŒ–ã—ã¦ã„ãã“ã¨ãŒäºˆæƒ³(æœŸå¾…)ã•ã‚Œã¾ã™ã€‚

æœ¬è¨˜äº‹ã‚’é¢ç™½ã„ã¨æ€ã£ã¦ãã‚ŒãŸæ–¹ãŒã„ã‚‰ã£ã—ã‚ƒã„ã¾ã—ãŸã‚‰ã€[news-recommendation-llm](https://github.com/YadaYuki/news-recommendation-llm)ã«ã‚¹ã‚¿ãƒ¼ã‚’ã—ã¦ãã ã•ã‚‹ã¨ã€åŠ±ã¿ã«ãªã‚Šã¾ã™ã€‚


# 7. å‚è€ƒæ–‡çŒ®

[1] "Empowering News Recommendation with Pre-Trained Language Models." Wu, C., Wu, F., Qi, T., & Huang, Y. https://doi.org/10.1145/3404835.3463069

[2] "Personalized News Recommendation: A Survey." Wu, C., Wu, F., & Huang, Y. https://arxiv.org/abs/2106.08934.

[3] "MIND: A Large-scale Dataset for News Recommendation" Wu, F., Qiao, Y., Chen, J.-H., Wu, C., Qi, T., Lian, J., Liu, D., Xie, X., Gao, J., Wu, W., & Zhou, M. https://aclanthology.org/2020.acl-main.331

[4] "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. https://aclanthology.org/N19-1423

[5] "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter" Sanh, V., Debut, L., Chaumond, J., & Wolf, T. https://arxiv.org/abs/1910.01108

[6] "Prompt Learning for News Recommendation" Zhang, Z., & Wang, B. arXiv preprint arXiv:2304.05263

[7] "ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models." Liu, Q., Chen, N., Sakai, T., & Wu, X.-M. arXiv:2305.06566.

[8] "Learning to Rank Using Gradient Descent" Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., & Hullender, G. https://doi.org/10.1145/1102351.1102363

[9] "Neural News Recommendation with Multi-Head Self-Attention" Wu, C., Wu, F., Ge, S., Qi, T., Huang, Y., & Xie, X. https://doi.org/10.18653/v1/D19-1671


# 8. Appendix




### Hyper parameters
|         Model          |  epoch  |  Learning Rate  | batch size | K(npratio) | history size |
| :--------------------: |  :----:  |  :----: | :----: | :----: | :----: |
| NRMS + DistilBERT-base |  3  |  1e-4  | 128 | 4 | 50 |
|    NRMS + BERT-base    |  3  |  1e-4  | 128 | 4 | 50 |


### Time to train & Trained Model

|         Model          |  Trained Model | Time to Train
| :--------------------: | :-----:| :-----: |
| NRMS + DistilBERT-base | [Google Drive](https://drive.google.com/file/d/1cw9WQSOVYJdYJCuIrSmU8odV2nsmith5/view) | 15.0h |
|    NRMS + BERT-base    | [Google Drive](https://drive.google.com/file/d/1ARiUgSVwcDFopFoIusp2MGQzwTMncOFf/view) | 28.5h |

