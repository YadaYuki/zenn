---
title: "Typescriptã§ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ"
emoji: "ğŸ§ "
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Typescript","Javascript","æ©Ÿæ¢°å­¦ç¿’","æ·±å±¤å­¦ç¿’"]
published: false
---

# 1.ã¯ã˜ã‚ã«
ä½œã‚‹çµŒé¨“ã¯ã‚³ãƒ”ãƒ¼ã§ããªã„ã€‚æŠ€è¡“æ›¸ã‚’èª­ã‚€ã ã‘ã§ã¯ã‚ã‹ã‚‰ãªã‹ã£ãŸã“ã¨ãŒã€ã€Œå®Ÿéš›ã«æ‰‹ã‚’å‹•ã‹ã—ã€ä½œã‚‹ã€ã¨ã„ã†ä½œæ¥­ã‚’çµŒã¦ã€ä¸€æœ¬ã®ç·šãŒã¤ãªãŒã‚‹ã‚ˆã†ã«ç†è§£ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚ãã‚“ãªçµŒé¨“ã¯ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã‚ã‚Œã°èª°ã‚‚ãŒä¸€åº¦ã¯ã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚

ã‚¿ã‚¤ãƒˆãƒ«ã«ã‚‚ã‚ã‚‹ã¨ãŠã‚Šã€ä»Šå›å–ã‚Šçµ„ã‚“ã ã“ã¨ã¯**Typescriptã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…**( ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯[MNIST](http://yann.lecun.com/exdb/mnist/) )ã§ã™ã€‚æœ€è¿‘ã€æ©Ÿæ¢°å­¦ç¿’ã®ç†è«–çš„èƒŒæ™¯ã‚’ã‚ˆã‚Šæ·±ãç†è§£ã—ãŸã„ãªãã€ã¨æ€ã£ã¦ã„ãŸã®ã§ã€å‹‰å¼·ãŒã¦ã‚‰ã‚„ã£ã¦ã¿ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚

# 2.ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learning

ãƒªãƒã‚¸ãƒˆãƒªã®[README](https://github.com/YadaYuki/neural_net_typescript#readme)ã«ã‚‚è¨˜è¿°ã•ã‚Œã¦ã„ã¾ã™ãŒã€ä»Šå›å®Ÿè£…ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å®Ÿè£…ãƒ»è¨­è¨ˆã¯ã€Œã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learningã€ã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹Pythonã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å®Ÿè£…ã«å¼·ãå½±éŸ¿ã‚’å—ã‘ã¦ã„ã¾ã™ã€‚

ã“ã“ã§ã€è›‡è¶³ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€Œã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learningã€ã¨ã„ã†æ›¸ç±ã«é–¢ã—ã¦èª¬æ˜ã—ã¦ã„ãã¾ã™ã€‚

https://www.oreilly.co.jp/books/9784873117584/

ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learningã€é€šç§°ã€Œã‚¼ãƒ­Dã€ã¯2021å¹´9æœˆæ™‚ç‚¹ã§ç™ºè¡Œéƒ¨æ•°ãŒ20ä¸‡ã‚’çªç ´ã™ã‚‹ãªã©ã€æŠ€è¡“æ›¸ã¨ã—ã¦ã¯ç•°ä¾‹ã®äººæ°—ã‚’èª‡ã£ã¦ã„ã¾ã™ã€‚

ã“ã®æœ¬ã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆã¯**å¤–éƒ¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«é ¼ã‚‰ãšã«ã€Python 3ã«ã‚ˆã£ã¦ã‚¼ãƒ­ã‹ã‚‰ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½œã‚‹ã“ã¨**ã€‚Pythonã®æ–‡æ³•ã‚„æ•°å­¦çš„ãªåŸºç¤çŸ¥è­˜ã‹ã‚‰å§‹ã¾ã‚Šã€æœ¬æ›¸å…¨ä½“ã‚’é€šã—ã¦ã€DNNã‚„CNNã¨ã„ã£ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚

æœ¬æ›¸ã¯ã€åƒ•ã®ã‚ˆã†ã«æ–°ã—ã„æŠ€è¡“ã‚’ç¿’å¾—ã™ã‚‹éš›ã€å®Ÿéš›ã«æ‰‹ã‚’å‹•ã‹ã—ãŸæ–¹ãŒåŠ¹ç‡è‰¯ãå­¦ã¹ã‚‹ã¨ã„ã†ã‚ˆã†ãªã‚¿ã‚¤ãƒ—ã«ã¯ã†ã£ã¦ã¤ã‘ã§ã™ã€‚ã“ã®æœ¬ã‚’å†™çµŒã™ã‚‹ã ã‘ã§ã‚‚ã€å¤šãã®çŸ¥è¦‹ãŒå¾—ã‚‰ã‚Œã‚‹ã¨æ€ã„ã¾ã™ã€‚ã—ã‹ã—ã€ãã‚Œã‚’å…¬é–‹ã—ãŸã¨ã“ã‚ã§å…¨ãé¢ç™½ãã‚ã‚Šã¾ã›ã‚“ã—ã€ãŸã å†™çµŒã™ã‚‹ã¨ã„ã†ã®ã¯æ¼«ç„¶ã¨ãªã‚ŠãŒã¡ã§ã€ã‚ˆã‚Šå¤šãã®çŸ¥è¦‹ã‚’å¾—ã‚‹ãŸã‚ã«ã¯ã€ã‚‚ã£ã¨å¿œç”¨çš„ãªã“ã¨ã«å–ã‚Šçµ„ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ã€Œã“ã®æœ¬ã®å†…å®¹ã‚’ã‚ˆã‚Šæ·±ãç†è§£ã™ã‚‹ãŸã‚ã«ã¯ã©ã†ã™ã‚Œã°è‰¯ã„ã‹...ã€è€ƒãˆãŸæœ«ã«å‡ºã—ãŸçµè«–ã¯ **æ©Ÿæ¢°å­¦ç¿’ã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã§ã‚ã‚‹Pythonä»¥å¤–ã®è¨€èªã‚’ç”¨ã„ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®è‡ªä½œã‚’ã™ã‚‹ã“ã¨** ã§ã—ãŸã€‚ãã—ã¦ã€**ä»Šå›æ¡ç”¨ã—ãŸã®ã¯Typescript**ã§ã™ã€‚

# 3.ãªãœTypescriptï¼Ÿ

ã“ã“ã§ã¯ä½¿ç”¨è¨€èªã¨ã—ã¦ã€Typescriptã‚’æ¡ç”¨ã—ãŸç†ç”±ã«ã¤ã„ã¦ã§ã™ã€‚

ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®é–‹ç™ºã‚’ã™ã‚‹ã“ã¨ãŒå¤šã„ã®ã§ã€ä½¿ã„æ…£ã‚Œã¦ã„ã‚‹ã¨ã„ã†ã®ã‚‚å¤§ããªç†ç”±ã®ä¸€ã¤ã§ã™ãŒã€ä»–ã®ç†ç”±ã¨ã—ã¦ã¯

- è¡Œåˆ—æ¼”ç®—ã«ãŠã‘ã‚‹æ¬¡å…ƒå‘¨è¾ºã®ãƒŸã‚¹ã‚’å‹ã«ã‚ˆã£ã¦é™çš„ã«å›é¿ã—ãŸã‹ã£ãŸã€‚
- ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ãªå®Ÿè£…ãŒå¯èƒ½ã§ã‚ã‚Šã€Deep Learningã¨ã®ç›¸æ€§ãŒè‰¯ã„ã€‚
- numjsã‚„tensorflow.js,math.jsãªã©è¡Œåˆ—æ¼”ç®—ãŒå¯èƒ½ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¤šãå­˜åœ¨ã™ã‚‹(ä»Šå›ã¯numjsã‚’æ¡ç”¨)

ã¨ã„ã£ãŸç‚¹ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚
 
 ä»¥ä¸Šã®èƒŒæ™¯ã‹ã‚‰ã€Typescriptã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ±ºå®šã—ã€å…¨çµåˆå‹ã®äºŒå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã„ã†2ç¨®é¡ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å®Ÿè£…ã«å–ã‚Šçµ„ã¿ã¾ã—ãŸã€‚æ¬¡ç« ä»¥é™ã§ã¯ãã‚Œãã‚Œã®æ§‹é€ ãƒ»å®Ÿè£…ãƒ»å®Ÿè¡Œçµæœã«ã¤ã„ã¦ã€ç´¹ä»‹ã—ã¦ã„ãã¾ã™ã€‚


# 4.äºŒå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯(TwoLayerNet)

ã¾ãšã€äºŒå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã§ã™ã€‚ä»Šå›å®Ÿè£…ã—ãŸäºŒå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹é€ ã‚’ã—ã¦ã„ã¾ã™ã€‚

![äºŒå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯](https://user-images.githubusercontent.com/57289763/132112979-2100d169-4fea-4d43-8d62-cac20570ac8f.png)

m=50ã®éš ã‚Œå±¤ãŒä¸€ã¤ã‚ã‚‹ã®ã¿ã¨ã„ã†éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹é€ ã®å…¨çµåˆå‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ãªãŠã€æ´»æ€§åŒ–é–¢æ•°ã¯ä¸­é–“å±¤ã¯Relué–¢æ•°ã€å‡ºåŠ›å±¤ã¯Softmaxé–¢æ•°ã‚’ç”¨ã„ã¦ã„ã¾ã™ã€‚

<!-- https://zenn.dev/ykyki/articles/math-formulae-in-zenn -->

- **Relu**
$$y = \left\{\begin{array}{ll}x & (x \gt 0) \\0 & (x \leq 0)\end{array}\right.$$
<!-- ref:https://medemanabu.net/latex/case-array-left-right/ -->
- **Softmax**

$$ y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)} $$

æ¬¡ã«ã€èª¤å·®é–¢æ•°ã§ã™ãŒã€ã“ã¡ã‚‰ã¯äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼èª¤å·®(Cross Entropy Error)ã‚’ç”¨ã„ã¦ã„ã¾ã™ã€‚

- **Cross Entropy Error**

$$E = - \sum_{k} t_k \log y_k $$

ãã‚Œã§ã¯Typescriptã«ã‚ˆã‚‹TwoLayerNetã®å®Ÿè£…ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
```
import nj from 'numjs';
import { Layer } from '../layers/base';
import { Affine } from '../layers/affine';
import { Relu } from '../layers/relu';
import { SoftmaxWithLoss } from '../layers/softmaxWithLoss';
import { softmax, softmaxBatch } from '../utils/activation';

export class TwoLayerNet {
  W1: nj.NdArray<number[]>;
  b1: nj.NdArray<number>;
  W2: nj.NdArray<number[]>;
  b2: nj.NdArray<number>;
  layers: Layer[];
  lossLayer: Layer;

  constructor(inputSize: number, hiddenSize: number, outputSize: number) {
    
    // å…¥åŠ›å±¤ â†’ ä¸­é–“å±¤ ã¸ã®é‡ã¿è¡Œåˆ—ãƒ»ãƒã‚¤ã‚¢ã‚¹ãƒ™ã‚¯ãƒˆãƒ«
    this.W1 = nj
      .random([inputSize * hiddenSize])
      .multiply(0.01)
      .reshape(inputSize, hiddenSize) as nj.NdArray<number[]>;
    this.b1 = nj.zeros([hiddenSize]);

    // ä¸­é–“å±¤ â†’ å‡ºåŠ›å±¤ ã¸ã®é‡ã¿è¡Œåˆ—ãƒ»ãƒã‚¤ã‚¢ã‚¹ãƒ™ã‚¯ãƒˆãƒ«
    this.W2 = nj
      .random([hiddenSize * outputSize])
      .multiply(0.01)
      .reshape(hiddenSize, outputSize) as nj.NdArray<number[]>;
    this.b2 = nj.zeros([outputSize]);
    
    // ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆæœ¬ä½“
    this.layers = [
      new Affine(this.W1, this.b1),
      new Relu(),
      new Affine(this.W2, this.b2),
    ];
    this.lossLayer = new SoftmaxWithLoss();
  }

  // äºˆæ¸¬
  predict(x: nj.NdArray<number>): nj.NdArray<number> {
    let output = x;
    for (const layer of this.layers) {
      output = layer.forward(output);
    }
    return softmax(output);
  }

  // é †ä¼æ¬
  forward(xBatch: nj.NdArray<number[]>, tBatch: nj.NdArray<number[]>): number {
    let scoreBatch: nj.NdArray<number[]> = xBatch;
    for (const layer of this.layers) {
      scoreBatch = layer.forwardBatch(scoreBatch);
    }
    const loss = this.lossLayer.forwardBatch(scoreBatch, tBatch);
    return loss;
  }

  // é€†ä¼æ¬
  backward(): void {
    let dout: nj.NdArray<number[]> = this.lossLayer.backwardBatch();
    const reversedLayers = this.layers.slice().reverse();
    for (const layer of reversedLayers) {
      dout = layer.backwardBatch(dout);
    }
  }

  // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°
  update(learningRate = 0.1): void {
    const { dW1, db1, dW2, db2 } = this.gradient();
    this.W1 = this.W1.subtract(dW1.multiply(learningRate));
    this.b1 = this.b1.subtract(db1.multiply(learningRate));
    this.W2 = this.W2.subtract(dW2.multiply(learningRate));
    this.b2 = this.b2.subtract(db2.multiply(learningRate));
    (this.layers[0] as Affine).W = this.W1;
    (this.layers[0] as Affine).b = this.b1;
    (this.layers[2] as Affine).W = this.W2;
    (this.layers[2] as Affine).b = this.b2;
  }

  // å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’è¿”ã™é–¢æ•°
  gradient(): {
    dW1: nj.NdArray<number[]>;
    db1: nj.NdArray<number>;
    dW2: nj.NdArray<number[]>;
    db2: nj.NdArray<number>;
  } {
    const affine1 = this.layers[0] as Affine;
    const affine2 = this.layers[2] as Affine;
    return {
      dW1: affine1.dW,
      db1: affine1.db,
      dW2: affine2.dW,
      db2: affine2.db,
    };
  }
}
```

TwoLayerNetã«mnistã®ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã¦ã¿ã¾ã™ã€‚å­¦ç¿’ã«ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```
$ yarn learn:tnn
```

å­¦ç¿’ç”¨ã®ã‚³ãƒ¼ãƒ‰ã¯ã“ã“ã§ã¯çœç•¥ã—ã¾ã™ãŒã€n=100(ãƒãƒƒãƒã‚µã‚¤ã‚º)ã€$\alpha$=0.1(å­¦ç¿’ç‡)ã¨ã„ã†æ¡ä»¶ã«ã‚ˆã‚‹ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã‚’è¡Œã£ã¦ãŠã‚Šã€100ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«ãã®æ™‚ç‚¹ã§ã®èª¤å·®é–¢æ•°ã®å‡ºåŠ›ã‚’è¡¨ç¤ºã™ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚ãã‚Œã§ã¯å®Ÿè¡Œçµæœã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```
$ yarn learn:tnn

Learn TwoLayer Neural Network...

iteration:1
2.301157570863233 # èª¤å·®é–¢æ•°ã®å‡ºåŠ›

iteration:101
1.6163854318773305

iteration:201
0.6387995662968232

iteration:301
0.4657332402247479

iteration:401
0.470693806907251

iteration:501
0.3244541415721461

iteration:601
0.27824362472848635

...
```

èª¤å·®é–¢æ•°ã®å‡ºåŠ›ãŒã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é‡ã­ã‚‹ã»ã©ã«æ¸›å°‘ã—ã¦ã„ã‚‹ã“ã¨ã‹ã‚‰ã€æ­£ã—ãå­¦ç¿’ã§ãã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

# 5.ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯(SimpleConvNet)

æ¬¡ã«ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã§ã™ã€‚ä»Šå›å®Ÿè£…ã—ãŸç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹é€ ã‚’ã—ã¦ã„ã¾ã™ã€‚

![ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯](https://user-images.githubusercontent.com/57289763/134599372-0c52ef9d-21ba-4096-8ef0-f7b1ae2ad894.png)

ç•³ã¿è¾¼ã¿å±¤ã¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ãŒä¸€ã¤ãšã¤ã€å…¨çµåˆå±¤ã‚’2ã¤æŒã¤è¨ˆå››å±¤ã®ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã™ã€‚

ãªãŠç•³ã¿è¾¼ã¿å±¤ã¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®é€šã‚Šã«ãªã‚Šã¾ã™ã€‚

- **ç•³ã¿è¾¼ã¿å±¤**

å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°:30
ç•³ã¿è¡Œåˆ—ã®ã‚µã‚¤ã‚º:5
ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰:1
ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°:0

- **ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤**

ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: 2 * 2
ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰:2
ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°:0


ã¾ãŸã€TwoLayerNetã¨åŒæ§˜ã€æ´»æ€§åŒ–é–¢æ•°ã¯ä¸­é–“å±¤ã¯Relué–¢æ•°ã€å‡ºåŠ›å±¤ã¯Softmaxé–¢æ•°ã‚’ç”¨ã„ã¦ãŠã‚Šã€èª¤å·®é–¢æ•°ã¯äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼èª¤å·®(Cross Entropy Error)ã‚’ç”¨ã„ã¦ã„ã¾ã™ã€‚

ãã‚Œã§ã¯ã€ä»¥ä¸Šã®ã“ã¨ã‚’è¸ã¾ãˆã¦ã€ä»Šå›å®Ÿè£…ã—ãŸç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯(SimpleConvNet)ã®å®Ÿè£…ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†

```
import nj from 'numjs';
import { Layer } from '../layers/base';
import { Affine } from '../layers/affine';
import { Relu } from '../layers/relu';
import { ImageRelu } from '../layers/imageRelu';
import { SoftmaxWithLoss } from '../layers/softmaxWithLoss';
import { Convolution } from '../layers/convolution';
import { Pooling } from '../layers/pooling';

export class SimpleConvNet {
  convW: nj.NdArray<number[][][]>;
  convB: nj.NdArray<number>;
  W1: nj.NdArray<number[]>;
  b1: nj.NdArray<number>;
  W2: nj.NdArray<number[]>;
  b2: nj.NdArray<number>;
  layers: Layer[];
  lossLayer: Layer;
  constructor(
    inputDim = { C: 1, Y: 28, X: 28 } as const, // MNISTã®å…¥åŠ›æ¬¡å…ƒ
    convParam = { filterNum: 30, filterSize: 5, pad: 0, stride: 1 } as const, // ç•³ã¿è¾¼ã¿å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    poolingParam = { poolH: 2, poolW: 2, pad: 0, stride: 2 } as const, // ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    hiddenSize = 100,
    outputSize = 10,
    weightInitStd = 0.01
  ) {
    const convOutputSize =
      (inputDim.Y - convParam.filterSize + 2 * convParam.pad) /
        convParam.stride +
      1;
    const poolOutputSize = Math.floor(
      convParam.filterNum * (convOutputSize / 2) * (convOutputSize / 2)
    );

    // ç•³ã¿è¾¼ã¿å±¤ã®é‡ã¿è¡Œåˆ—ãƒ»ãƒã‚¤ã‚¢ã‚¹ãƒ™ã‚¯ãƒˆãƒ«
    this.convW = nj
      .random([
        convParam.filterNum,
        inputDim.C,
        convParam.filterSize,
        convParam.filterSize,
      ])
      .multiply(weightInitStd)
      .reshape(
        convParam.filterNum,
        inputDim.C,
        convParam.filterSize,
        convParam.filterSize
      );
    this.convB = nj.random(convParam.filterNum).multiply(weightInitStd);

    // ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ â†’ ä¸­é–“å±¤ã¸ã®é‡ã¿è¡Œåˆ—ãƒ»ãƒã‚¤ã‚¢ã‚¹ãƒ™ã‚¯ãƒˆãƒ«
    this.W1 = nj
      .random([poolOutputSize * hiddenSize])
      .multiply(weightInitStd)
      .reshape(poolOutputSize, hiddenSize) as nj.NdArray<number[]>;
    this.b1 = nj.zeros([hiddenSize]);

    // ä¸­é–“å±¤ â†’ å‡ºåŠ›å±¤ã¸ã®é‡ã¿è¡Œåˆ—ãƒ»ãƒã‚¤ã‚¢ã‚¹ãƒ™ã‚¯ãƒˆãƒ«
    this.W2 = nj
      .random([hiddenSize * outputSize])
      .multiply(weightInitStd)
      .reshape(hiddenSize, outputSize) as nj.NdArray<number[]>;
    this.b2 = nj.zeros([outputSize]);

    // ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆæœ¬ä½“ã€‚
    this.layers = [
      new Convolution(this.convW, this.convB, convParam.stride, convParam.pad),
      new ImageRelu(),
      new Pooling(
        poolingParam.poolH,
        poolingParam.poolW,
        poolingParam.stride,
        poolingParam.pad
      ),
      new Affine(this.W1, this.b1),
      new Relu(),
      new Affine(this.W2, this.b2),
    ];
    this.lossLayer = new SoftmaxWithLoss();
  }

  // é †ä¼æ¬
  forward(
    xBatch: nj.NdArray<number[][][]>,
    tBatch: nj.NdArray<number[]>
  ): number {
    let scoreBatch: nj.NdArray<number[][][] | number[]> = xBatch;
    for (const layer of this.layers) {
      scoreBatch = layer.forwardBatch(scoreBatch);
    }
    const loss = this.lossLayer.forwardBatch(scoreBatch, tBatch);
    return loss;
  }

  // é€†ä¼æ¬
  backward(): void {
    let dout: nj.NdArray<number[] | number[][][]> =
      this.lossLayer.backwardBatch();
    const reversedLayers = this.layers.slice().reverse();
    for (const layer of reversedLayers) {
      dout = layer.backwardBatch(dout);
    }
  }

  // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°
  update(learningRate = 0.1): void {
    const { dConvW, dConvB, dW1, db1, dW2, db2 } = this.gradient();
    this.convW = this.convW.subtract(dConvW.multiply(learningRate));
    this.convB = this.convB.subtract(dConvB.multiply(learningRate));
    this.W1 = this.W1.subtract(dW1.multiply(learningRate));
    this.b1 = this.b1.subtract(db1.multiply(learningRate));
    this.W2 = this.W2.subtract(dW2.multiply(learningRate));
    this.b2 = this.b2.subtract(db2.multiply(learningRate));
    (this.layers[0] as Convolution).W = this.convW;
    (this.layers[0] as Convolution).b = this.convB;
    (this.layers[3] as Affine).W = this.W1;
    (this.layers[3] as Affine).b = this.b1;
    (this.layers[5] as Affine).W = this.W2;
    (this.layers[5] as Affine).b = this.b2;
  }

  // å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’è¿”ã™é–¢æ•°
  gradient(): {
    dConvW: nj.NdArray<number[][][]>;
    dConvB: nj.NdArray<number>;
    dW1: nj.NdArray<number[]>;
    db1: nj.NdArray<number>;
    dW2: nj.NdArray<number[]>;
    db2: nj.NdArray<number>;
  } {
    const conv = this.layers[0] as Convolution;
    const affine1 = this.layers[3] as Affine;
    const affine2 = this.layers[5] as Affine;
    return {
      dConvW: conv.dW,
      dConvB: conv.db,
      dW1: affine1.dW,
      db1: affine1.db,
      dW2: affine2.dW,
      db2: affine2.db,
    };
  }
}
```

ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’ã‚‚å®Ÿè¡Œã—ã¦ã¿ã¾ã™ã€‚å­¦ç¿’ã¯```yarn learn:cnn```ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§å‹•ä½œã—ã¾ã™

```
$ yarn learn:cnn

Learn Convolutional Neural Network...

iteration: 1
2.302419697001273 # èª¤å·®é–¢æ•°ã®å‡ºåŠ›

iteration: 101
2.1932988463612713

iteration: 201
1.9089445618023353

iteration: 301
1.001005840921999

iteration: 401
0.42583633641717195

iteration: 501
0.38454650943764324

iteration: 601
0.43363565201204224

iteration: 701
0.19343118291118838

...
```

èª¤å·®é–¢æ•°ã®å‡ºåŠ›ãŒã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é‡ã­ã‚‹ã»ã©ã«æ¸›å°‘ã—ã¦ã„ã‚‹ã“ã¨ã‹ã‚‰ã€ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã«é–¢ã—ã¦ã‚‚ã€æ­£ã—ãå­¦ç¿’ã§ãã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

# 6.ã¾ã¨ã‚

ä»Šå›ã¯Typescriptã‚’ç”¨ã„ãŸå…¨çµåˆå‹ã®äºŒå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®Ÿè£…ã«å–ã‚Šçµ„ã¿ã¾ã—ãŸã€‚

å‹å®‰å…¨æ€§ã‚’æ±‚ã‚ã€Typescriptãƒ»numjsã¨ã„ã†æŠ€è¡“é¸å®šã‚’ã—ã¾ã—ãŸãŒã€å®Ÿè¡Œé€Ÿåº¦ã‚„æ¼”ç®—å‡¦ç†ãŒã‚ˆã‚Šç°¡æ½”ã«è¨˜è¿°ã§ããŸã‚Šã™ã‚‹ç‚¹ã§Pythonãƒ»numpyã®æ–¹ãŒå„ªã‚Œã¦ãŠã‚Šã€æ©Ÿæ¢°å­¦ç¿’åˆ†é‡ã«ãŠã‘ã‚‹Pythonä¸€å¼·ã¯ã—ã°ã‚‰ãç¶šããã†ã§ã™ã€‚

ã—ã‹ã—ã€ã‚¼ãƒ­ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’å®Ÿè£…ã—ã¦ã¿ãŸã“ã¨ã¯ã€è‡ªåˆ†ã®ä¸­ã§ã€æ©Ÿæ¢°å­¦ç¿’ã®ç†è«–ãªçŸ¥è­˜ã®æ•´ç†ã«å¤§ããè²¢çŒ®ã—ã¦ãã‚ŒãŸã‚ˆã†ã«æ„Ÿã˜ã¦ã„ã¾ã™ã€‚ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learningã¯èª­ã‚€ã ã‘ã§ã‚‚å¤§å¤‰å‹‰å¼·ã«ãªã‚Šã¾ã™ãŒã€ãã“ã§å¾—ãŸçŸ¥è­˜ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸå¤šè¨€èªã§ã®ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã‚‚éå¸¸ã«é¢ç™½ã„ä½œæ¥­ãªã®ã§ã€èˆˆå‘³ãŒã‚ã‚‹æ–¹ã¯æ˜¯éè©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚

# 7.å‚è€ƒæ–‡çŒ®

https://www.oreilly.co.jp/books/9784873117584/

https://tutorials.chainer.org/ja/13_Basics_of_Neural_Networks.html
