---
title: "Typescriptでゼロから作るニューラルネット"
emoji: "🧠"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["Typescript","javascript","機械学習","深層学習"]
published: false
---

# 1.はじめに
作る経験はコピーできない。技術書を読むだけではわからなかったことが、「実際に手を動かし、作る」という作業を経て、一本の線がつながるように理解できるようになった。そんな経験は、エンジニアであれば誰もが一度はあると思います。

タイトルにもあるとおり、今回取り組んだことは**Typescriptによるニューラルネットのスクラッチ実装**( データセットは[MNIST](http://yann.lecun.com/exdb/mnist/) )です。最近、機械学習の理論的背景をより深く理解したいなぁ、と思っていたので、勉強がてらやってみることにしました。

# 2.ゼロから作るDeep Learning

リポジトリの[README](https://github.com/YadaYuki/neural_net_typescript#readme)にも記述されていますが、今回実装したニューラルネットの実装・設計は「ゼロから作るDeep Learning」に掲載されているPythonによるニューラルネットの実装に強く影響を受けています。

ここで蛇足かもしれませんが「ゼロから作るDeep Learning」という書籍に関して説明していきます。

https://www.oreilly.co.jp/books/9784873117584/

ゼロから作るDeep Learning、通称「ゼロD」は2021年9月時点で発行部数が20万を突破するなど、技術書としては異例の人気を誇っています。

この本のコンセプトは**外部のライブラリに頼らずに、Python 3によってゼロからディープラーニングを作ること**。Pythonの文法や数学的な基礎知識から始まり、本書全体を通して、DNNやCNNといったアルゴリズムをゼロから作ることができるように構成されています。

本書は、僕のように新しい技術を習得する際、実際に手を動かした方が効率良く学べるというようなタイプにはうってつけです。この本を写経するだけでも、多くの知見が得られると思います。しかし、それを公開したところで全く面白くありませんし、ただ写経するというのは漫然となりがちで、より多くの知見を得るためには、もっと応用的なことに取り組む必要があります。

「この本の内容をより深く理解するためにはどうすれば良いか...」考えた末に出した結論は **機械学習のデファクトであるPython以外の言語を用いてニューラルネットの自作をすること** でした。そして、**今回採用したのはTypescript**です。

# 3.なぜTypescript？

ここでは使用言語として、Typescriptを採用した理由についてです。

フロントエンドの開発をすることが多いので、使い慣れているというのも大きな理由の一つですが、他の理由としては

- 行列演算における次元周辺のミスを型によって静的に回避したかった。
- オブジェクト指向な実装が可能であり、Deep Learningとの相性が良い。
- numjsやtensorflow.js,math.jsなど行列演算が可能なライブラリが多く存在する(今回はnumjsを採用)

といった点が挙げられます。
 
 以上の背景から、Typescriptを使用することを決定し、二層ニューラルネットワークと畳み込みニューラルネットワークという2つの実装に取り組みました。次章以降ではそれぞれの構造・実装・実行結果について、紹介していきます。


# 4.二層ニューラルネットワーク(TwoLayerNet)

まず、二層ニューラルネットワークについてです。今回実装した二層ニューラルネットは以下のような構造をしています。

![二層ニューラルネットワーク](https://user-images.githubusercontent.com/57289763/132112979-2100d169-4fea-4d43-8d62-cac20570ac8f.png)

m=50の隠れ層が一つあるのみという非常にシンプルな構造のニューラルネットです。なお、活性化関数は中間層はRelu関数、出力層はSoftmax関数を用いています。

- **Relu**

// 数式

- **Softmax**

// 数式

次に、誤差関数ですが、こちらは交差エントロピー誤差(Cross Entropy Error)を用いています。

- **Cross Entropy Error**

// 数式

それではTypescriptによるTwoLayerNetの実装を見てみましょう。
```
import nj from 'numjs';
import { Layer } from '../layers/base';
import { Affine } from '../layers/affine';
import { Relu } from '../layers/relu';
import { SoftmaxWithLoss } from '../layers/softmaxWithLoss';
import { softmax, softmaxBatch } from '../utils/activation';

export class TwoLayerNet {
  W1: nj.NdArray<number[]>;
  b1: nj.NdArray<number>;
  W2: nj.NdArray<number[]>;
  b2: nj.NdArray<number>;
  layers: Layer[];
  lossLayer: Layer;
  constructor(inputSize: number, hiddenSize: number, outputSize: number) {
    this.W1 = nj
      .random([inputSize * hiddenSize])
      .multiply(0.01)
      .reshape(inputSize, hiddenSize) as nj.NdArray<number[]>;
    this.b1 = nj.zeros([hiddenSize]);
    this.W2 = nj
      .random([hiddenSize * outputSize])
      .multiply(0.01)
      .reshape(hiddenSize, outputSize) as nj.NdArray<number[]>;
    this.b2 = nj.zeros([outputSize]);
    this.layers = [
      new Affine(this.W1, this.b1),
      new Relu(),
      new Affine(this.W2, this.b2),
    ];
    this.lossLayer = new SoftmaxWithLoss();
  }

  predict(x: nj.NdArray<number>): nj.NdArray<number> {
    let output = x;
    for (const layer of this.layers) {
      output = layer.forward(output);
    }
    return softmax(output);
  }

  forward(xBatch: nj.NdArray<number[]>, tBatch: nj.NdArray<number[]>): number {
    let scoreBatch: nj.NdArray<number[]> = xBatch;
    for (const layer of this.layers) {
      scoreBatch = layer.forwardBatch(scoreBatch);
    }
    const loss = this.lossLayer.forwardBatch(scoreBatch, tBatch);
    return loss;
  }
  backward(): void {
    let dout: nj.NdArray<number[]> = this.lossLayer.backwardBatch();
    const reversedLayers = this.layers.slice().reverse();
    for (const layer of reversedLayers) {
      dout = layer.backwardBatch(dout);
    }
  }

  update(learningRate = 0.1): void {
    const { dW1, db1, dW2, db2 } = this.gradient();
    this.W1 = this.W1.subtract(dW1.multiply(learningRate));
    this.b1 = this.b1.subtract(db1.multiply(learningRate));
    this.W2 = this.W2.subtract(dW2.multiply(learningRate));
    this.b2 = this.b2.subtract(db2.multiply(learningRate));
    (this.layers[0] as Affine).W = this.W1;
    (this.layers[0] as Affine).b = this.b1;
    (this.layers[2] as Affine).W = this.W2;
    (this.layers[2] as Affine).b = this.b2;
  }

  gradient(): {
    dW1: nj.NdArray<number[]>;
    db1: nj.NdArray<number>;
    dW2: nj.NdArray<number[]>;
    db2: nj.NdArray<number>;
  } {
    const affine1 = this.layers[0] as Affine;
    const affine2 = this.layers[2] as Affine;
    return {
      dW1: affine1.dW,
      db1: affine1.db,
      dW2: affine2.dW,
      db2: affine2.db,
    };
  }
}
```

// 変数の役割とか。関数の役割とかを簡単に説明

TwoLayerNetにmnistのデータを学習させてみます。学習には以下のコマンドを実行します。
```
$ yarn learn:tnn
```

学習用のコードはここでは省略しますが、n=100(バッチサイズ)、$\alpha$=0.1(学習率)という条件によるミニバッチ学習を行っており、100イテレーションごとにその時点での誤差関数の出力を表示するようにしています。それでは実行結果を見てみましょう。

```
$ yarn learn:tnn

Learn TwoLayer Neural Network...

iteration:1
2.301157570863233 # 誤差関数の出力

iteration:101
1.6163854318773305

iteration:201
0.6387995662968232

iteration:301
0.4657332402247479

iteration:401
0.470693806907251

iteration:501
0.3244541415721461

iteration:601
0.27824362472848635

...
```

誤差関数の出力がイテレーションを重ねるほどに減少していることから、正しく学習できていることがわかります。

# 5.畳み込みニューラルネットワーク(SimpleConvNet)

# 6.まとめ

# 7.参考文献

https://www.oreilly.co.jp/books/9784873117584/


