---
title: "PyTorchã§è‡ªä½œã—ã¦ç†è§£ã™ã‚‹Transformer"
emoji: "ğŸ™Š"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["python","pytorch","æ©Ÿæ¢°å­¦ç¿’","transformer"]
published: false
---



## 1. ã¯ã˜ã‚ã«

Transformerã¯2017å¹´ã«ã€ŒAttention is all you needã€ã¨ã„ã†è«–æ–‡ã§ç™ºè¡¨ã•ã‚Œã€è‡ªç„¶è¨€èªå‡¦ç†ç•Œã«ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’å·»ãèµ·ã“ã—ãŸæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è«–æ–‡å†…ã§ã¯ã€è‹±èªâ†’ãƒ‰ã‚¤ãƒ„èªç¿»è¨³ãƒ»è‹±èªâ†’ãƒ•ãƒ©ãƒ³ã‚¹èªç¿»è¨³ã¨ã„ã†äºŒã¤ã®æ©Ÿæ¢°ç¿»è¨³ã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚ãã‚Œã¾ã§æœ€ã‚‚é«˜ã„ç²¾åº¦ã‚’å‡ºã™ã¨ã•ã‚Œã¦ã„ãŸCNN,RNN(LSTM)ãƒ™ãƒ¼ã‚¹ã®æ©Ÿæ¢°ç¿»è¨³ã¨æ¯”è¼ƒã—ã¦ã€

- ç²¾åº¦(Bleuã‚¹ã‚³ã‚¢)
- è¨“ç·´ã«ã‹ã‹ã‚‹ã‚³ã‚¹ãƒˆã®å°‘ãªã•

ã¨ã„ã†ä¸¡æ–¹ã®é¢ã§ã€Transformerã¯ãã‚Œã‚‰ã®æ€§èƒ½ã‚’ä¸Šå›ã‚Šã¾ã—ãŸã€‚ä»¥é™ã€Transformerã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸæ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ãã®ä¾‹ã¨ã—ã¦ã¯ã€BERT,XLNet,GPT-3ã¨ã„ã£ãŸè¿‘å¹´ã®SoTAã¨ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚

ã“ã“ã§ã€ã€ŒAttention is all you needã€å†…ã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹Transformerã®æ§‹é€ ã®å›³ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†

![Transformer](https://user-images.githubusercontent.com/57289763/160270884-e1901241-a1e6-4890-a5e8-165e87f0c4da.png)

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ©Ÿæ¢°ç¿»è¨³ã«ãŠã‘ã‚‹Transformerã¯**ã‚ã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’åˆ¥ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹(Ex:æ—¥æœ¬èªã«ã‚ˆã‚‹æ–‡ç« ã‚’è‹±èªã«ã‚ˆã‚‹æ–‡ç« ã«ç¿»è¨³ã™ã‚‹)ã‚ˆã†ãªã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚‰ã‚Œã‚‹Encoder-Decoder(seq2seq)ã®æ§‹é€ ã‚’ã—ã¦ã„ã‚‹**ã¨ã„ã†ç‚¹ã§ã¯RNN/LSTMãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ã§ã™ã€‚

ã—ã‹ã—ã€Transformerã®**æœ€å¤§ã®ç‰¹å¾´ã¯Encoderãƒ»Decoderã®ã„ãšã‚Œã«ã‚‚RNNã‚„LSTMã®ã‚ˆã†ãªå†å¸°è¨ˆç®—ã‚’å¿…è¦ã¨ã™ã‚‹å±¤ãŒå­˜åœ¨ã›ãšã€ãã®ä»£ã‚ã‚Šã¨ã—ã¦ã“ã®å¾Œèª¬æ˜ã™ã‚‹AttentionãŒç”¨ã„ã‚‰ã‚Œã¦ã„ã‚‹ç‚¹**ã§ã™ã€‚

Transformerã¯è‡ªç„¶è¨€èªå‡¦ç†ã®ã¿ãªã‚‰ãšã€ä»–ã®åˆ†é‡ã§ã‚‚ç”¨ã„ã‚‰ã‚Œã‚‹æ±ç”¨æ€§ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã¾ãŸ2022å¹´ç¾åœ¨ã€ç™»å ´ã‹ã‚‰ç´„5å¹´ã¨ã„ã†æœˆæ—¥ãŒçµŒéã—ã¦ã„ã‚‹ãŸã‚ã€ä¸»è¦ãªãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹[Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)ãƒ»[Tensorflow](https://tensorflow.github.io/tensor2tensor/#language-modeling)ã®ã„ãšã‚Œã«ã‚‚æ—¢ã«å…¬å¼å®Ÿè£…ãŒå­˜åœ¨ã—ã¦ãŠã‚Šã€ç ”ç©¶ç­‰ã‚’ç›®çš„ã«å®Ÿè£…ã™ã‚‹éš›ã¯ã“ã‚Œã‚‰ã‚’ç”¨ã„ã‚‹ã®ãŒä¸€èˆ¬çš„ã‹ã¨æ€ã„ã¾ã™ã€‚

ã—ã‹ã—ã€ä½œã‚‹ã“ã¨ã¯ç†è§£ã™ã‚‹ã“ã¨ã¸ã®è¿‘é“ã€‚ã¨ã„ã†ã“ã¨ã§ã€ä»Šå›ã¯å–ã‚Šçµ„ã‚“ã ã®ã¯Transformerã¨Transformerã‚’æ§‹æˆã™ã‚‹å±¤ã®ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã§ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ã€Transformerãƒ¢ãƒ‡ãƒ«ã‚’æ§‹æˆã™ã‚‹å„ãƒ¬ã‚¤ãƒ¤ã®ç†è«–çš„èƒŒæ™¯ãŠã‚ˆã³Pytorchã«ã‚ˆã‚‹å®Ÿè£…ã‚’ç´¹ä»‹ã—ã¦ã„ãã¾ã™ã€‚

ãªãŠã€ä»Šå›ã¯Pytorchã®å­¦ç¿’ã‚‚å…¼ã­ã¦ã„ã‚‹ãŸã‚ã€[PytorchLightning](https://www.pytorchlightning.ai/)ã‚„[Ignite](https://pytorch.org/ignite/index.html)ã®ã‚ˆã†ãªä¾¿åˆ©ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ä½¿ç”¨ã›ãšã€ç´ ã®Pytorchã®ã¿ã§å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚

ä»¥ä¸‹ã¯å®Ÿè£…ã—ãŸãƒªãƒã‚¸ãƒˆãƒªã«ãªã‚Šã¾ã™ã€‚

https://github.com/YadaYuki/en_ja_translator_pytorch
 


## 2. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆæ¦‚è¦³

ãã‚Œã§ã¯ã€æ—©é€Ÿå®Ÿè£…ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚ç´°éƒ¨ã«æ³¨ç›®ã™ã‚‹å‰ã«ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…¨ä½“åƒã‚’æ¦‚è¦³ã™ã‚‹ãŸã‚ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆã‚’è¦‹ã¦ã¿ã¾ã™ã€‚

```
.
â”œâ”€â”€ const // pathãªã©ã®å®šæ•°å€¤
â”‚Â Â  â””â”€â”€ path.py
â”œâ”€â”€ corpus // è¨“ç·´ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚³ãƒ¼ãƒ‘ã‚¹ãŒå…¥ã‚‹
â”‚Â Â  â””â”€â”€ kftt-data-1.0
â”œâ”€â”€ figure
â”œâ”€â”€ layers // æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’æ§‹æˆã™ã‚‹ãƒ¬ã‚¤ãƒ¤ã®å®Ÿè£…
â”‚Â Â  â””â”€â”€ transformer
â”‚Â Â      â”œâ”€â”€ Embedding.py
â”‚Â Â      â”œâ”€â”€ FFN.py
â”‚Â Â      â”œâ”€â”€ MultiHeadAttention.py
â”‚Â Â      â”œâ”€â”€ PositionalEncoding.py
â”‚Â Â      â”œâ”€â”€ ScaledDotProductAttention.py
â”‚Â Â      â”œâ”€â”€ TransformerDecoder.py
â”‚Â Â      â””â”€â”€ TransformerEncoder.py
â”œâ”€â”€ models // æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
â”‚Â Â  â”œâ”€â”€ Transformer.py
â”‚Â Â  â””â”€â”€ __init__.py
â”œâ”€â”€ mypy.ini
â”œâ”€â”€ pickles // ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®pickleãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ ¼ç´
â”‚Â Â  â””â”€â”€ nn/
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ poetry.toml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ tests // ãƒ†ã‚¹ãƒˆ(pytest)
â”‚Â Â  â”œâ”€â”€ conftest.py
â”‚Â Â  â”œâ”€â”€ layers/
â”‚Â Â  â”œâ”€â”€ models/
â”‚Â Â  â””â”€â”€ utils/
â”œâ”€â”€ train.py // è¨“ç·´ç”¨ã‚³ãƒ¼ãƒ‰
â””â”€â”€ utils // Datasetã‚„Vocabã¨ã„ã£ãŸã‚¯ãƒ©ã‚¹ã®å®Ÿè£…,å‰å‡¦ç†ã«ç”¨ã„ã‚‹é–¢æ•°ã®å®Ÿè£…
    â”œâ”€â”€ dataset/
    â”œâ”€â”€ download.py
    â”œâ”€â”€ evaluation/
    â””â”€â”€ text/
```

`poetry.*`ã¨ã„ã†åå‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‹ã‚‰ã‚ã‹ã‚‹é€šã‚Šã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†ã«ã¯[Poetry](https://python-poetry.org/)ã‚’æ¡ç”¨ã—ã¾ã—ãŸã€‚Poetryã«ã¯ã€

- `poetry install`ã‚³ãƒãƒ³ãƒ‰ä¸€ã¤ã§å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å…¨ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
- é–‹ç™ºæ™‚ã®ã¿å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨é–‹ç™ºæ™‚ã¨æœ¬ç•ªç’°å¢ƒã®ä¸¡æ–¹ã§å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä¸€ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«(pyproject.toml)ã§å®šç¾©ã§ãã‚‹.

ãªã©ã®ãƒ¡ãƒªãƒƒãƒˆãŒã‚ã‚Šã¾ã™.

ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã¨ã—ã¦ã¯ã€`models`ä»¥ä¸‹ã«ã€ŒTransformeræœ¬ä½“ã€ã€`layers`ä»¥ä¸‹ã«ã€ŒTransformerã‚’æ§‹æˆã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€ã¨ã„ã†æ¬¡ç« ã§èª¬æ˜ã™ã‚‹å®Ÿè£…ã«è©²å½“ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã™ã€‚

## 3. Transformerã‚’æ§‹æˆã™ã‚‹å±¤

ãã‚Œã§ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…¨ä½“åƒãŒæ´ã‚ãŸã¨ã“ã‚ã§ã€å„å±¤ã®å®Ÿè£…ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚
## 3.1 Attention

å…ˆã»ã©ã‚‚è¿°ã¹ãŸé€šã‚Šã€Transformerã®æœ€å¤§ã®ç‰¹å¾´ã¯**RNNã®ã‚ˆã†ã«è¨“ç·´æ™‚ã®å†å¸°è¨ˆç®—ã‚’è¡Œã‚ãšã«ã€Attentionå±¤ã‚’ç”¨ã„ã¦ã„ã‚‹ç‚¹**ã§ã™ã€‚ã€ŒAttention is all you needã€ã¨ã„ã†è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ãŒç‰©èªã£ã¦ã„ã‚‹ã‚ˆã†ã«ã€Attentionã¯Transformerã®ä¸­ã§æœ€ã‚‚é‡è¦ãªå±¤ã§ã™ã€‚ã“ã“ã§ã¯ã€Transformerã®æ ¸ã¨ãªã‚‹å±¤ã§ã‚ã‚‹Attentionå±¤ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

Attentionå±¤ã®ç›®çš„ã¯ã€å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã®å„è¦ç´ ã®é‡è¦åº¦ã‚’ç®—å‡ºã—ã€ãã‚Œã«ã‚ˆã£ã¦å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã‚’é‡ã¿ä»˜ã‘ã‚‹ã“ã¨ã€‚Attentionå±¤ã§ã¯ã€**ç”»åƒã‚„ãƒ†ã‚­ã‚¹ãƒˆã¨ã„ã£ãŸå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«ã®ä¸­ã§ã€æ­£ã—ã„å‡ºåŠ›ã‚’å¾—ã‚‹ãŸã‚ã«é‡è¦ãªè¦ç´ ã¯ã©ã‚Œã§ã‚ã‚‹ã‹ï¼Ÿã¨ã„ã†é‡è¦åº¦ã®å½¹å‰²ã‚’æœãŸã™Attention Weightã¨ã„ã†å€¤ã‚’ç®—å‡ºã—ã¾ã™**ã€‚

ã€Œå…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã€ã¨ã€Œç®—å‡ºã—ãŸAttention Weightã€ã®ç©ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€**å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã®ä¸­ã§æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’å¾—ã‚‹ä¸Šã§ç‰¹ã«é‡è¦ãªè¦ç´ ã‚’é‡ã¿ä»˜ã‘**ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

Attentionã‚’å®šå¼åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãšã¯ã€é‡è¦åº¦ã‚’è¡¨ã™Attention Weightã®è¨ˆç®—ã§ã™ã€‚Attention Weightã‚’æ±‚ã‚ã‚‹é–¢æ•°$\alpha$ã‚’ç”¨ã„ã¦å…¥åŠ›Xã¨Q(ã‚¯ã‚¨ãƒª)ã‹ã‚‰ã€ Xã®é‡è¦åº¦ã‚’æ±‚ã‚ã¾ã™ã€‚

$$ Attention Weight = \alpha(Q,X) $$

ä¸Šã®å¼ã§æ±‚ã‚ãŸAttention Weight(Xã®é‡è¦åº¦)ã§å…¥åŠ›Xã‚’é‡ã¿ä»˜ã‘ãŸã‚‚ã®ãŒAttentionã®å‡ºåŠ›ã«ãªã‚Šã¾ã™ã€‚

$$Output = (Attention Weight)X =\alpha(Q,X)X $$

ã“ã“ã§ã¯ã€ã€ŒAttention Weightã‚’ç®—å‡ºã—ã€ãã‚Œã«ã‚ˆã‚Šå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒé‡ã¿ä»˜ã‘ã‚‰ã‚Œã‚‹ã€ã¨ã„ã†æ¼”ç®—ã®æµã‚ŒãŒã‚ã‹ã‚‹ã‚ˆã†ã«ã€ä¸Šã®ã‚ˆã†ãªå¼ã§Attentionå±¤ã§ã®æ¼”ç®—ã‚’è¡¨ã—ã¾ã—ãŸã€‚ã—ã‹ã—ã€æœ¬å®¶ã®è«–æ–‡å†…ã§ã¯Attentionã®æ¼”ç®—ã‚’ã€Q(ã‚¯ã‚¨ãƒª),K(ã‚­ãƒ¼),V(ãƒãƒªãƒ¥ãƒ¼)ã¨ã„ã†3ã¤ã®è¨˜å·ã‚’ç”¨ã„ã¦ã€

$$ Attention(Q,K,V) $$

ã®ã‚ˆã†ã«è¡¨ã—ã¦ã„ã¾ã™.å…ˆç¨‹ã®$\alpha(Q,X)X$ã¨$Attention(Q,K,V)$ã¯ã„ãšã‚Œã‚‚Attentionã®æ¼”ç®—ã§ã‚ã‚‹ãŸã‚ã€æ›¸ãæ–¹ãŒç•°ãªã‚‹ã ã‘ã§ã€ã‚„ã£ã¦ã„ã‚‹ã“ã¨ã¯å…¨ãåŒã˜ã§ã™ã€‚

$\alpha(Q,X)X$ã‚’$Attention(Q,K,V)$ã«ã‚ˆã£ã¦è¡¨ã™ã¨ä»¥ä¸‹ã®é€šã‚Šã«ãªã‚Šã¾ã™.

$$Attention(Q,K,V) = Attention(Q,X,X) = \alpha(Q,X)X$$

ä»¥ä¸ŠãŒAttentionå±¤ã®æ¦‚è¦ã«é–¢ã™ã‚‹èª¬æ˜ã§ã™ã€‚

ã•ã¦ã€æœ¬ç¯€ã§ã¯ã€Attentionå±¤ãŒè¡Œã†æ¼”ç®—ã®æ¦‚è¦ã¨ãã®ç›®çš„ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã—ãŸã€‚ã—ã‹ã—è‚å¿ƒãªã®ã¯ã€**å…¥åŠ›ã®é‡è¦åº¦ã‚’è¡¨ã™Attention Weightã‚’ã©ã®ã‚ˆã†ã«ç®—å‡ºã™ã¹ãã§ã‚ã‚‹ã‹ï¼Ÿã¨ã„ã†ç‚¹ã§ã™(é–¢æ•°$\alpha$ãŒä½•ã§ã‚ã‚‹ã‹ï¼Ÿ)**ã€‚

Transformerã§ã¯**ã‚¯ã‚¨ãƒªQ,å…¥åŠ›ãƒ‡ãƒ¼ã‚¿Xã®å†…ç©ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€Attention Weight**ã‚’ç®—å‡ºã—ã¦ã„ã¾ã™ã€‚ãã®ã‚ˆã†ãªAttentionå±¤ã‚’**ScaledDotProductAttention**ã¨å‘¼ã³ã¾ã™ã€‚

## 3.2 ScaledDotProductAttention

Transformerã§ç”¨ã„ã‚‰ã‚Œã‚‹Attentionã§ã‚ã‚‹ScaledDotProductAttentionã®Attention Weightè¨ˆç®—ã¯ã€ã‚¯ã‚¨ãƒªQ($N{\times}D$)ã¨å…¥åŠ›X($N{\times}D$)ã‚’ç”¨ã„ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå¼ã§è¡¨ã™ã“ã¨ãŒã§ãã¾ã™.

$$\alpha(Q,X) = softmax(\frac{QX^T}{\sqrt{D}})$$

è³ªå•å¿œç­”ã‚„æ©Ÿæ¢°ç¿»è¨³ã¨ã„ã£ãŸã‚¿ã‚¹ã‚¯ã«Transformerãƒ¢ãƒ‡ãƒ«ã§å–ã‚Šçµ„ã‚“ã§ã„ã‚‹å ´åˆã€ä¸Šã®å¼ã«ãŠã‘ã‚‹Q,Xã¯ãã‚Œãã‚Œæ–‡ç« ãƒ‡ãƒ¼ã‚¿ã‚’è¡Œåˆ—ã§è¡¨ç¾ã—ãŸã‚‚ã®ã§ã™ã€‚æ‰±ã†ãƒ‡ãƒ¼ã‚¿ãŒNå€‹ã®å˜èªã‚’æŒã¤æ–‡ç« ã‚’ã€Dæ¬¡å…ƒã®å˜èªåˆ†æ•£è¡¨ç¾ã§è¡¨ç¾ã—ãŸãƒ‡ãƒ¼ã‚¿ã§ã‚ã£ãŸå ´åˆã€$N{\times}D$ã¨ã„ã†ã‚µã‚¤ã‚ºã‚’æŒã¤è¡Œåˆ—ã¨ãªã£ã¦ã„ã¾ã™. 

ãã®ãŸã‚ã€ã“ã®æ¼”ç®—ã§ã¯ã€ã‚¯ã‚¨ãƒªQå†…ã®å„å˜èªã®åˆ†æ•£è¡¨ç¾ã¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿Xå†…ã®å„å˜èªã®åˆ†æ•£è¡¨ç¾ã®å†…ç©ã‚’ç®—å‡ºã—ã¦ã„ã‚‹ã€ã¨ã„ã†ã“ã¨ã§ã™ã€‚ãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®å†…ç©ãŒå¤§ãã„ã¨ã„ã†ã“ã¨ã¯ã€å‘ã„ã¦ã„ã‚‹æ–¹å‘ãŒè¿‘ã„ã€ã™ãªã‚ã¡ã€ãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®é¡ä¼¼åº¦ãŒé«˜ã„ã¨ã„ã†ã“ã¨ã§ã™(å˜èªåŒå£«ã®é¡ä¼¼åº¦ãŒé«˜ã„)ã€‚ã¤ã¾ã‚Šã€ScaledDotProductAttentionã«æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã—ãŸå ´åˆã€Q,Xå†…ã®å˜èªåŒå£«ã®é¡ä¼¼åº¦ã‚’ã€å…¥åŠ›ã«å¯¾ã™ã‚‹é‡è¦æ€§ã¨ã—ã¦é‡ã¿ä»˜ã‘ã¦ã„ã‚‹ã¨è§£é‡ˆã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

ä¸Šã§æ±‚ã‚ãŸAttention Weightã¨å…¥åŠ›Xã®ç©ã‚’æ±‚ã‚ã‚‹ã“ã¨ã§æœ€çµ‚çš„ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚å¾“ã£ã¦ã€å…¥åŠ›X,ã‚¯ã‚¨ãƒªQã«å¯¾ã™ã‚‹ScaledDotProductAttentionã®å‡ºåŠ›ã¯ä»¥ä¸‹ã®å¼ã§è¡¨ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

$$Attention(Q,K,V) = Attention(Q,X,X) = \alpha(Q,X)X = softmax(\frac{QX^T}{\sqrt{D}})X$$

ãã‚Œã§ã¯ã€ ScaledDotProductAttentionã®Pytorchã«ã‚ˆã‚‹å®Ÿè£…ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†.

```python
import numpy as np
import torch
from torch import nn


class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k: int) -> None:
        super().__init__()
        self.d_k = d_k

    def forward(
        self,
        q: torch.Tensor,  # =Q
        k: torch.Tensor,  # =X
        v: torch.Tensor,  # =X
        mask: torch.Tensor = None,
    ) -> torch.Tensor:
        scalar = np.sqrt(self.d_k)
        attention_weight = torch.matmul(q, torch.transpose(k, 1, 2)) / scalar # ã€ŒQ * X^T / (D^0.5)ã€" ã‚’è¨ˆç®—

        if mask is not None: # maskã«å¯¾ã™ã‚‹å‡¦ç†
            if mask.dim() != attention_weight.dim():
                raise ValueError(
                    "mask.dim != attention_weight.dim, mask.dim={}, attention_weight.dim={}".format(
                        mask.dim(), attention_weight.dim()
                    )
                )
            attention_weight = attention_weight.data.masked_fill_(
                mask, -torch.finfo(torch.float).max
            ) 

        attention_weight = nn.functional.softmax(attention_weight, dim=2) # Attention Weightã‚’è¨ˆç®—
        return torch.matmul(attention_weight, v) # (Attention Weight) * X ã«ã‚ˆã‚Šé‡ã¿ä»˜ã‘.
```


## 3.3 Multihead Attention

3.2ç¯€ã§ã€Transformerãƒ¢ãƒ‡ãƒ«ã¯Attentionã®è¨ˆç®—æ–¹æ³•ã¨ã—ã¦ScaledDotProductAttentionã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’èª¬æ˜ã—ã¾ã—ãŸã€‚

ã—ã‹ã—ã€Transformerã§æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹Attentionã¯å˜ãªã‚‹ScaledDotProductAttentionã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å®Ÿéš›ã®Transformerã§ã¯å˜ä¸€ã®å…¥åŠ›ã«å¯¾ã—ã¦ã€**è¤‡æ•°ã®ScaledDotProductAttentionã‚’ä¸¦åˆ—ã§å®Ÿè¡Œã™ã‚‹Multihead Attention**ã¨ã„ã†ä»•çµ„ã¿ãŒæ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

ã“ã“ã§ã€æœ¬å®¶ã®è«–æ–‡ã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹Multihead Attentionã®æ¦‚è¦å›³ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

![Multihead Attention](https://user-images.githubusercontent.com/57289763/160265954-9451fb41-3906-4f29-8e74-d4255925141c.png)

å›³ä¸­ã®h(ãƒ˜ãƒƒãƒ‰æ•°)ã¯ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ScaledDotProductAttentionã®æ•°ã‚’è¡¨ã—ã¾ã™.

Multihead Attentionã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå‡¦ç†ãŒè¡Œã‚ã‚Œã¾ã™ã€‚
1. Attentionå±¤ã«å¯¾ã™ã‚‹å…¥åŠ›$Q$($N_{Q}$$\times$$d_{model}$),$K$($N$$\times$$d_{model}$),$V$($N$$\times$$d_{model}$)ã‚’$h$(ãƒ˜ãƒƒãƒ‰æ•°)ã®æ•°ã ã‘è¤‡è£½ã™ã‚‹
2. è¤‡è£½ã—ãŸå…¥åŠ›$Q_i$,$K_i$,$V_i$(i=1~h)ã‚’è¡Œåˆ—$W_i^q$($d_{model}$$\times$$d_k$),$W_i^k$($d_{model}$$\times$$d_k$),$W_i^v$($d_{model}$$\times$$d_v$)ã«ã‚ˆã‚Šã€$d_{model}$â†’$d_v$,$d_k$ã¸ã¨ç·šå½¢å¤‰æ›ã™ã‚‹ã€‚
3. ãã†ã—ã¦å¾—ã‚‰ã‚ŒãŸ$Q_i$$W_i^q$($N_Q$$\times$$d_k$),$K_i$$W^k_i$($N$$\times$$d_k$),$V_i$$W^v_i$($N$$\times$$d_v$)ã‚’hå€‹å­˜åœ¨ã™ã‚‹ScaledDotProductAttentionã¸å…¥åŠ›ã™ã‚‹
4. ä¸¦åˆ—å®Ÿè¡Œã•ã‚ŒãŸScaledDotProductAttentionã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹hå€‹ã®å‡ºåŠ›head(i=1~h,$N$$\times$$d_v$)ã‚’çµåˆ(concat)ã—ã€è¡Œåˆ—$O$($N$$\times$$hd_v$)ã‚’å¾—ã‚‹ã€‚
5. $O$$W^O$ã«ã‚ˆã‚Š$O$ã‚’$hd_v$â†’$d_{model}$ã«ç·šå½¢å¤‰æ›ã—ã€å¾—ã‚‰ã‚ŒãŸå€¤ãŒæœ€çµ‚çš„ãªå‡ºåŠ›ã¨ãªã‚‹ã€‚

å®šå¼åŒ–ã™ã‚‹ã¨ä»¥ä¸‹ã®é€šã‚Šã«ãªã‚Šã¾ã™ã€‚


$$ head_i = ScaledDotProductAttention(Q_iW^q_i,K_iW^k_i,V_iW^v_i) (i = 1 \sim h)$$
$$ O = Concat(head_1, ..., head_h) $$
$$ MultiHead(Q,K,V) = OW^O $$


ãã‚Œã§ã¯ã€ä»¥ä¸Šã‚’è¸ã¾ãˆã¦ã€å®Ÿè£…ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†.
```python
import torch
from layers.transformer.ScaledDotProductAttention import ScaledDotProductAttention
from torch import nn


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, h: int) -> None:
        super().__init__()
        self.d_model = d_model
        self.h = h
        self.d_k = d_model // h
        self.d_v = d_model // h

        #
        self.W_k = nn.Parameter(
            torch.Tensor(h, d_model, self.d_k)  # ãƒ˜ãƒƒãƒ‰æ•°, å…¥åŠ›æ¬¡å…ƒ, å‡ºåŠ›æ¬¡å…ƒ(=å…¥åŠ›æ¬¡å…ƒ/ãƒ˜ãƒƒãƒ‰æ•°)
        )

        self.W_q = nn.Parameter(
            torch.Tensor(h, d_model, self.d_k)  # ãƒ˜ãƒƒãƒ‰æ•°, å…¥åŠ›æ¬¡å…ƒ, å‡ºåŠ›æ¬¡å…ƒ(=å…¥åŠ›æ¬¡å…ƒ/ãƒ˜ãƒƒãƒ‰æ•°)
        )

        self.W_v = nn.Parameter(
            torch.Tensor(h, d_model, self.d_v)  # ãƒ˜ãƒƒãƒ‰æ•°, å…¥åŠ›æ¬¡å…ƒ, å‡ºåŠ›æ¬¡å…ƒ(=å…¥åŠ›æ¬¡å…ƒ/ãƒ˜ãƒƒãƒ‰æ•°)
        )

        self.scaled_dot_product_attention = ScaledDotProductAttention(self.d_k)

        self.linear = nn.Linear(h * self.d_v, d_model)

    def forward(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        mask_3d: torch.Tensor = None,
    ) -> torch.Tensor:

        batch_size, seq_len = q.size(0), q.size(1)

        """repeat Query,Key,Value by num of heads"""
        q = q.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model
        k = k.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model
        v = v.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model

        """Linear before scaled dot product attention"""
        q = torch.einsum(
            "hijk,hkl->hijl", (q, self.W_q)
        )  # head, batch_size, d_k, seq_len
        k = torch.einsum(
            "hijk,hkl->hijl", (k, self.W_k)
        )  # head, batch_size, d_k, seq_len
        v = torch.einsum(
            "hijk,hkl->hijl", (v, self.W_v)
        )  # head, batch_size, d_k, seq_len

        """Split heads"""
        q = q.view(self.h * batch_size, seq_len, self.d_k)
        k = k.view(self.h * batch_size, seq_len, self.d_k)
        v = v.view(self.h * batch_size, seq_len, self.d_v)

        if mask_3d is not None:
            mask_3d = mask_3d.repeat(self.h, 1, 1)

        """Scaled dot product attention"""
        attention_output = self.scaled_dot_product_attention(
            q, k, v, mask_3d
        )  # (head*batch_size, seq_len, d_model)

        attention_output = torch.chunk(attention_output, self.h, dim=0)
        attention_output = torch.cat(attention_output, dim=2)

        """Linear after scaled dot product attention"""
        output = self.linear(attention_output)
        return output
```


## 3.4 PositionalEncoding

## 3.5 Position-wise Feed-Forward Networks

## 3.6 Encoder
## 3.7 Decoder
## 3.8 Transformer(å®Œæˆç‰ˆ)
## .è‹±èª â†’ æ—¥æœ¬èªç¿»è¨³æ©Ÿã®å­¦ç¿’

ã•ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ãŒå®Œäº†ã—ãŸã®ã§ã€ã„ã‚ˆã„ã‚ˆãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ç§»ã‚Šã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã«ã‚ãŸã£ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’ç›®çš„ã¨ã—ãŸã‚¯ãƒ©ã‚¹ãƒ»Trainerã‚’[train.py](https://github.com/YadaYuki/en_ja_translator_pytorch/blob/master/train.py#L22)ã«å®šç¾©ã—ã¾ã—ãŸã€‚ãªãŠã€Trainerã‚¯ãƒ©ã‚¹ã¯Pytorch Lightningã®APIã‚’å‚è€ƒã«ã—ã¦ãŠã‚Šã€ä»¥ä¸‹ã®5ã¤ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŒã¡ã¾ã™ã€‚

- loss_fn: èª¤å·®é–¢æ•°ã®è¨ˆç®—
- train_step: ãƒãƒƒãƒå­¦ç¿’ã«ãŠã‘ã‚‹1ã‚¹ãƒ†ãƒƒãƒ—(è¨“ç·´)
- val_step: ãƒãƒƒãƒå­¦ç¿’ã«ãŠã‘ã‚‹1ã‚¹ãƒ†ãƒƒãƒ—(æ¤œè¨¼)
- fit: ãƒãƒƒãƒå­¦ç¿’ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ»æ¤œè¨¼ã‚’è¡Œã†
- test: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ã‚’è¡Œã†

ã¨ã„ã†ã‚ã‘ã§ã€å®Ÿè£…ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚Trainerã‚¯ãƒ©ã‚¹ã®å®Ÿè£…ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™(ãã‚Œãªã‚Šã«é‡ãŒã‚ã‚‹ãŸã‚ã‚¢ã‚³ãƒ¼ãƒ‡ã‚£ã‚ªãƒ³ã«ã—ã¦ã‚ã‚Šã¾ã™)

:::details Trainerã‚¯ãƒ©ã‚¹
```python
from os.path import join
from typing import List, Tuple

import torch
from matplotlib import pyplot as plt
from torch import nn, optim
from torch.utils.data import DataLoader

from const.path import (
    FIGURE_PATH,
    KFTT_TOK_CORPUS_PATH,
    NN_MODEL_PICKLES_PATH,
    TANAKA_CORPUS_PATH,
)
from models import Transformer
from utils.dataset.Dataset import KfttDataset
from utils.evaluation.bleu import BleuScore
from utils.text.text import tensor_to_text, text_to_tensor
from utils.text.vocab import get_vocab


class Trainer:
    def __init__(
        self,
        net: nn.Module,
        optimizer: optim.Optimizer,
        critetion: nn.Module,
        bleu_score: BleuScore,
        device: torch.device,
    ) -> None:
        self.net = net
        self.optimizer = optimizer
        self.critetion = critetion
        self.device = device
        self.bleu_score = bleu_score
        self.net = self.net.to(self.device)

    def loss_fn(self, preds: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        return self.critetion(preds, labels)

    def train_step(
        self, src: torch.Tensor, tgt: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, float]:
        self.net.train()
        output = self.net(src, tgt)

        tgt = tgt[:, 1:]  # decoderã‹ã‚‰ã®å‡ºåŠ›ã¯1 ~ max_lenã¾ã§ãªã®ã§ã€0ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã§èª¤å·®é–¢æ•°ã‚’è¨ˆç®—ã™ã‚‹
        output = output[:, :-1, :]  #

        # calculate loss
        loss = self.loss_fn(
            output.contiguous().view(
                -1,
                output.size(-1),
            ),
            tgt.contiguous().view(-1),
        )

        # calculate bleu score
        _, output_ids = torch.max(output, dim=-1)
        bleu_score = self.bleu_score(tgt, output_ids)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss, output, bleu_score

    def val_step(
        self, src: torch.Tensor, tgt: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, float]:
        self.net.eval()
        output = self.net(src, tgt)

        tgt = tgt[:, 1:]
        output = output[:, :-1, :]  #

        loss = self.loss_fn(
            output.contiguous().view(
                -1,
                output.size(-1),
            ),
            tgt.contiguous().view(-1),
        )
        _, output_ids = torch.max(output, dim=-1)
        bleu_score = self.bleu_score(tgt, output_ids)

        return loss, output, bleu_score

    def fit(
        self, train_loader: DataLoader, val_loader: DataLoader, print_log: bool = True
    ) -> Tuple[List[float], List[float], List[float], List[float]]:
        # train
        train_losses: List[float] = []
        train_bleu_scores: List[float] = []
        if print_log:
            print(f"{'-'*20 + 'Train' + '-'*20} \n")
        for i, (src, tgt) in enumerate(train_loader):
            src = src.to(self.device)
            tgt = tgt.to(self.device)
            loss, _, bleu_score = self.train_step(src, tgt)
            src = src.to("cpu")
            tgt = tgt.to("cpu")

            if print_log:
                print(
                    f"train loss: {loss.item()}, bleu score: {bleu_score},"
                    + f"iter: {i+1}/{len(train_loader)} \n"
                )

            train_losses.append(loss.item())
            train_bleu_scores.append(bleu_score)

        # validation
        val_losses: List[float] = []
        val_bleu_scores: List[float] = []
        if print_log:
            print(f"{'-'*20 + 'Validation' + '-'*20} \n")
        for i, (src, tgt) in enumerate(val_loader):
            src = src.to(self.device)
            tgt = tgt.to(self.device)
            loss, _, bleu_score = self.val_step(src, tgt)
            src = src.to("cpu")
            tgt = tgt.to("cpu")

            if print_log:
                print(f"train loss: {loss.item()}, iter: {i+1}/{len(val_loader)} \n")

            val_losses.append(loss.item())
            val_bleu_scores.append(bleu_score)

        return train_losses, train_bleu_scores, val_losses, val_bleu_scores

    def test(self, test_data_loader: DataLoader) -> Tuple[List[float], List[float]]:
        test_losses: List[float] = []
        test_bleu_scores: List[float] = []
        for i, (src, tgt) in enumerate(test_data_loader):
            src = src.to(self.device)
            tgt = tgt.to(self.device)
            loss, _, bleu_score = trainer.val_step(src, tgt)
            src = src.to("cpu")
            tgt = tgt.to("cpu")

            test_losses.append(loss.item())
            test_bleu_scores.append(bleu_score)

        return test_losses, test_bleu_scores
```
:::

ãã‚Œã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¦ã„ãã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```
$ poetry run python train.py
```

ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¾ã™.

```
$ poetry run python train.py

epoch: 1
--------------------Train--------------------

train loss: 10.104473114013672, bleu score: 0.0,iter: 1/4403

train loss: 9.551202774047852, bleu score: 0.0,iter: 2/4403

train loss: 8.950608253479004, bleu score: 0.0,iter: 3/4403

train loss: 8.688143730163574, bleu score: 0.0,iter: 4/4403

train loss: 8.4220552444458, bleu score: 0.0,iter: 5/4403

train loss: 8.243291854858398, bleu score: 0.0,iter: 6/4403

train loss: 8.187620162963867, bleu score: 0.0,iter: 7/4403

train loss: 7.6360859870910645, bleu score: 0.0,iter: 8/4403

....
```

ä»¥ä¸‹ã¯1ã‚¹ãƒ†ãƒƒãƒ—å½“ãŸã‚Šã®è¨“ç·´ãƒ­ã‚¹ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ãŸã‚°ãƒ©ãƒ•ã§ã™ã€‚

![image](https://user-images.githubusercontent.com/57289763/160236001-929f9221-12bd-464e-9c0e-29222228cd89.png)

å®Ÿè¡Œç’°å¢ƒã®éƒ½åˆã§ã€ã€Œãƒ¢ãƒ‡ãƒ«ã‚’ååˆ†ã«è¨“ç·´ã—ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹Bleuã‚¹ã‚³ã‚¢ãƒ»Lossã§æ¤œè¨¼ã‚’è¡Œã†ã€ã¨ã„ã†æ¤œè¨¼ã¯ã§ãã¾ã›ã‚“ã§ã—ãŸãŒ(ã™ã¿ã¾ã›ã‚“ğŸ™‡â€â™‚ï¸)ã€1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šã®è¨“ç·´ãƒ­ã‚¹ãŒé †èª¿ã«æ¸›ã£ã¦ã„ã‚‹ã“ã¨ã‹ã‚‰ã€æ­£ã—ãå­¦ç¿’ã§ãã¦ã„ã‚‹ã¨æ¨å¯Ÿã•ã‚Œã¾ã™ã€‚

## .ã¾ã¨ã‚
## .å‚è€ƒæ–‡çŒ®

https://arxiv.org/abs/1706.03762

https://www.amazon.co.jp/%E8%A9%B3%E8%A7%A3%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0-TensorFlow-Keras%E3%83%BBPyTorch%E3%81%AB%E3%82%88%E3%82%8B%E6%99%82%E7%B3%BB%E5%88%97%E3%83%87%E3%83%BC%E3%82%BF%E5%87%A6%E7%90%86-Compass-Books%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA/dp/4839969515

https://www.amazon.co.jp/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92-%E6%94%B9%E8%A8%82%E7%AC%AC2%E7%89%88-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E5%B2%A1%E8%B0%B7-%E8%B2%B4%E4%B9%8B/dp/4065133327

https://www.amazon.co.jp/PyTorch%E5%AE%9F%E8%B7%B5%E5%85%A5%E9%96%80-Eli-Stevens/dp/4839974691

https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E7%B7%A8-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873118360/ref=pd_bxgy_img_1/358-0651022-5160614?pd_rd_w=JO4Kw&pf_rd_p=020fee25-8ced-4191-bce3-27e7ce0c0e3b&pf_rd_r=WM1R7J4578P1B0MCNSJE&pd_rd_r=2b8a68ac-2514-4675-9132-acafd1cf2853&pd_rd_wg=kDhDj&pd_rd_i=4873118360&psc=1

https://qiita.com/halhorn/items/c91497522be27bde17ce

https://deeplearning.hatenablog.com/entry/transformer

