---
title: "PyTorchで自作して理解するTransformer"
emoji: "🙊"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["python","pytorch","機械学習","transformer"]
published: false
---



## 1. はじめに
Transformerは2017年に「Attention is all you need」という論文で発表され、自然言語処理界にブレイクスルーを巻き起こした深層学習モデルです。論文内では、英語→ドイツ語翻訳・英語→フランス語翻訳という二つの機械翻訳タスクによる性能評価が行われています。それまで最も高い精度を出すとされていたCNN,RNN(LSTM)ベースの機械翻訳と比較して、

- 精度(Bleuスコア)
- 訓練にかかるコストの少なさ

という両方の面で、Transformerはそれらの性能を上回りました。以降、Transformerをベースとした様々なモデルが提案されています。その例としては、BERT,XLNet,GPT-3といった近年のSoTAとされているモデルが挙げられます。

Transformerは自然言語処理のみならず、他の分野でも用いられる汎用性の高いモデルです。そのため、主要なディープラーニングフレームワークである[Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)・[Tensorflow](https://tensorflow.github.io/tensor2tensor/#language-modeling)のいずれにも既に公式実装が存在しており、研究等を目的に実装する際はこれらを用いるのが一般的かと思います。

しかし、作ることは理解することへの近道。ということで、今回は取り組んだのはTransformerとTransformerを構成する層のスクラッチ実装です。本記事では、Transformerモデルを構成する各レイヤの理論的背景およびPytorchによる実装を紹介していきます。以下は実装したリポジトリになります。

https://github.com/YadaYuki/en_ja_translator_pytorch
 
## 2. Transformerとは？
	
## 3. ディレクトリ構成概観

際部に注目する前に、プロジェクトの全体像を概観することは非常に重要です。ということで、ディレクトリ構成は以下の通りになります。

```
.
├── const
│   └── path.py
├── corpus
│   └── kftt-data-1.0
├── figure
├── layers
│   └── transformer
│       ├── Embedding.py
│       ├── FFN.py
│       ├── MultiHeadAttention.py
│       ├── PositionalEncoding.py
│       ├── ScaledDotProductAttention.py
│       ├── TransformerDecoder.py
│       └── TransformerEncoder.py
├── models
│   ├── Transformer.py
│   └── __init__.py
├── mypy.ini
├── pickles
│   └── nn/
├── poetry.lock
├── poetry.toml
├── pyproject.toml
├── tests
│   ├── conftest.py
│   ├── layers/
│   ├── models/
│   └── utils/
├── train.py
└── utils
    ├── dataset/
    ├── download.py
    ├── evaluation/
    └── text/
```

`models`以下に「Transformer本体」、`layers`以下に「Transformerを構成するレイヤー」という次章で説明する実装に該当するファイルが存在します。

また、`poetry.*`という名前のファイルが存在することからわかる通り、ライブラリのパッケージ管理にはPoetryを採用しました。Poetryには、現時点で僕が把握しているだけでも、

- `poetry install`コマンド一つで必要なライブラリを全てインストールし、仮想環境を作成することができる.
- 開発時のみ必要なライブラリと開発時と本番環境の両方で必要なライブラリを一つのファイル(pyproject.toml)で定義できる.

などのメリットがあります.(Python版のnpm・yarnのようなイメージです)

## 4. Transformerを構成する層
## 4.1 Attention
## 4.2 ScaledDotProductAttention
## 4.3 Multihead Attention
## 4.4 PositionalEncoding
## 4.5 Position-wise Feed-Forward Networks
## 4.6 Encoder
## 4.7 Decoder
## 4.8 Transformer(完成版)
## 5.英語 → 日本語翻訳機の学習
## 6.まとめ
## 7.参考文献

https://arxiv.org/abs/1706.03762

https://www.amazon.co.jp/%E8%A9%B3%E8%A7%A3%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0-TensorFlow-Keras%E3%83%BBPyTorch%E3%81%AB%E3%82%88%E3%82%8B%E6%99%82%E7%B3%BB%E5%88%97%E3%83%87%E3%83%BC%E3%82%BF%E5%87%A6%E7%90%86-Compass-Books%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA/dp/4839969515

https://www.amazon.co.jp/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92-%E6%94%B9%E8%A8%82%E7%AC%AC2%E7%89%88-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E5%B2%A1%E8%B0%B7-%E8%B2%B4%E4%B9%8B/dp/4065133327

https://www.amazon.co.jp/PyTorch%E5%AE%9F%E8%B7%B5%E5%85%A5%E9%96%80-Eli-Stevens/dp/4839974691

https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E7%B7%A8-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873118360/ref=pd_bxgy_img_1/358-0651022-5160614?pd_rd_w=JO4Kw&pf_rd_p=020fee25-8ced-4191-bce3-27e7ce0c0e3b&pf_rd_r=WM1R7J4578P1B0MCNSJE&pd_rd_r=2b8a68ac-2514-4675-9132-acafd1cf2853&pd_rd_wg=kDhDj&pd_rd_i=4873118360&psc=1

https://qiita.com/halhorn/items/c91497522be27bde17ce

https://deeplearning.hatenablog.com/entry/transformer

