---
title: "PyTorchで自作して理解するTransformer"
emoji: "🙊"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["python","pytorch","機械学習","transformer"]
published: false
---



## 1. はじめに

Transformerは2017年に「Attention is all you need」という論文で発表され、自然言語処理界にブレイクスルーを巻き起こした深層学習モデルです。論文内では、英語→ドイツ語翻訳・英語→フランス語翻訳という二つの機械翻訳タスクによる性能評価が行われています。それまで最も高い精度を出すとされていたCNN,RNN(LSTM)ベースの機械翻訳と比較して、

- 精度(Bleuスコア)
- 訓練にかかるコストの少なさ

という両方の面で、Transformerはそれらの性能を上回りました。以降、Transformerをベースとした様々なモデルが提案されています。その例としては、BERT,XLNet,GPT-3といった近年のSoTAとされているモデルが挙げられます。

ここで、「Attention is all you need」内に掲載されているTransformerの構造の図を見てみましょう

![Transformer](https://user-images.githubusercontent.com/57289763/160270884-e1901241-a1e6-4890-a5e8-165e87f0c4da.png)

ニューラル機械翻訳におけるTransformerは**ある時系列データを別の時系列データに変換する(Ex:日本語による文章を英語による文章に翻訳する)ようなタスクに用いられるEncoder-Decoder(seq2seq)の構造をしている**という点ではRNN/LSTMベースのモデルと同じです。

しかし、Transformerの**最大の特徴はEncoder・DecoderのいずれにもRNNやLSTMのような再帰計算を必要とする層が存在せず、その代わりとしてこの後説明するAttentionが用いられている点**です。

Transformerは自然言語処理のみならず、他の分野でも用いられる汎用性の高いモデルです。また2022年現在、登場から約5年という月日が経過しているため、主要なディープラーニングフレームワークである[Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)・[Tensorflow](https://tensorflow.github.io/tensor2tensor/#language-modeling)のいずれにも既に公式実装が存在しており、研究等を目的に実装する際はこれらを用いるのが一般的かと思います。

しかし、作ることは理解することへの近道。ということで、今回は取り組んだのはTransformerとTransformerを構成する層のスクラッチ実装です。本記事では、Transformerモデルを構成する各レイヤの理論的背景およびPytorchによる実装を紹介していきます。

なお、今回はPytorchの学習も兼ねているため、[PytorchLightning](https://www.pytorchlightning.ai/)や[Ignite](https://pytorch.org/ignite/index.html)のような便利ライブラリは使用せず、素のPytorchのみで実装しています。

以下は実装したリポジトリになります。

https://github.com/YadaYuki/en_ja_translator_pytorch
 


## 2. ディレクトリ構成概観

それでは、早速実装を見ていきましょう。細部に注目する前に、プロジェクトの全体像を概観するため、ディレクトリ構成を見てみます。

```
.
├── const // pathなどの定数値
│   └── path.py
├── corpus // 訓練用のデータ・コーパスが入る
│   └── kftt-data-1.0
├── figure
├── layers // 深層ニューラルネットを構成するレイヤの実装
│   └── transformer
│       ├── Embedding.py
│       ├── FFN.py
│       ├── MultiHeadAttention.py
│       ├── PositionalEncoding.py
│       ├── ScaledDotProductAttention.py
│       ├── TransformerDecoder.py
│       └── TransformerEncoder.py
├── models // 深層ニューラルネットモデルの実装
│   ├── Transformer.py
│   └── __init__.py
├── mypy.ini
├── pickles // モデルやデータセットのpickleファイルを格納
│   └── nn/
├── poetry.lock
├── poetry.toml
├── pyproject.toml
├── tests // テスト(pytest)
│   ├── conftest.py
│   ├── layers/
│   ├── models/
│   └── utils/
├── train.py // 訓練用コード
└── utils // DatasetやVocabといったクラスの実装,前処理に用いる関数の実装
    ├── dataset/
    ├── download.py
    ├── evaluation/
    └── text/
```

`poetry.*`という名前のファイルが存在することからわかる通り、ライブラリのパッケージ管理には[Poetry](https://python-poetry.org/)を採用しました。Poetryには、

- `poetry install`コマンド一つで必要なライブラリを全てインストールし、仮想環境を作成することができる.
- 開発時のみ必要なライブラリと開発時と本番環境の両方で必要なライブラリを一つのファイル(pyproject.toml)で定義できる.

などのメリットがあります.

フォルダ構成としては、`models`以下に「Transformer本体」、`layers`以下に「Transformerを構成するレイヤー」という次章で説明する実装に該当するファイルが存在します。

## 3. Transformerを構成する層

それではプロジェクトの全体像が掴めたところで、各層の実装を見ていきましょう。
## 3.1 Attention

先ほども述べた通り、Transformerの最大の特徴は**RNNのように訓練時の再帰計算を行わずに、Attention層を用いている点**です。「Attention is all you need」という論文のタイトルが物語っているように、AttentionはTransformerの中で最も重要な層です。ここでは、Transformerの核となる層であるAttention層について説明します。

Attention層の目的は、入力ベクトルの各要素の重要度を算出し、それによって入力ベクトルを重み付けること。Attention層では、**画像やテキストといった入力データのベクトルの中で、正しい出力を得るために重要な要素はどれであるか？という重要度の役割を果たすAttention Weightという値を算出します**。

「入力ベクトル」と「算出したAttention Weight」の積を計算することにより、**入力ベクトルの中で正解ラベルを得る上で特に重要な要素を重み付け**ることが可能です。

Attentionを定式化してみましょう。まずは、重要度を表すAttention Weightの計算です。Attention Weightを求める関数$\alpha$を用いて入力XとQ(クエリ)から、 Xの重要度を求めます。

$$ Attention Weight = \alpha(Q,X) $$

上の式で求めたAttention Weight(Xの重要度)で入力Xを重み付けたものがAttentionの出力になります。

$$Output = (Attention Weight)X =\alpha(Q,X)X $$

ここでは、「Attention Weightを算出し、それにより入力データが重み付けられる」という演算の流れがわかるように、上のような式でAttention層での演算を表しました。しかし、本家の論文内ではAttentionの演算を、Q(クエリ),K(キー),V(バリュー)という3つの記号を用いて、

$$ Attention(Q,K,V) $$

のように表しています.先程の$\alpha(Q,X)X$と$Attention(Q,K,V)$はいずれもAttentionの演算であるため、書き方が異なるだけで、やっていることは全く同じです。

$\alpha(Q,X)X$を$Attention(Q,K,V)$によって表すと以下の通りになります.

$$Attention(Q,K,V) = Attention(Q,X,X) = \alpha(Q,X)X$$

以上がAttention層の概要に関する説明です。

さて、本節では、Attention層が行う演算の概要とその目的について説明しました。しかし肝心なのは、**入力の重要度を表すAttention Weightをどのように算出すべきであるか？という点です(関数$\alpha$が何であるか？)**。

Transformerでは**クエリQ,入力データXの内積を計算することによって、Attention Weight**を算出しています。そのようなAttention層を**ScaledDotProductAttention**と呼びます。

## 3.2 ScaledDotProductAttention

Transformerで用いられるAttentionであるScaledDotProductAttentionのAttention Weight計算は、クエリQ($N{\times}D$)と入力X($N{\times}D$)を用いて、以下のような式で表すことができます.

$$\alpha(Q,X) = softmax(\frac{QX^T}{\sqrt{D}})$$

質問応答や機械翻訳といったタスクにTransformerモデルで取り組んでいる場合、上の式におけるQ,Xはそれぞれ文章データを行列で表現したものです。扱うデータがN個の単語を持つ文章を、D次元の単語分散表現で表現したデータであった場合、$N{\times}D$というサイズを持つ行列となっています. 

そのため、この演算では、クエリQ内の各単語の分散表現と入力データX内の各単語の分散表現の内積を算出している、ということです。ベクトル同士の内積が大きいということは、向いている方向が近い、すなわち、ベクトル同士の類似度が高いということです(単語同士の類似度が高い)。つまり、ScaledDotProductAttentionに文章データを入力した場合、Q,X内の単語同士の類似度を、入力に対する重要性として重み付けていると解釈することが可能です。

上で求めたAttention Weightと入力Xの積を求めることで最終的な出力が得られます。従って、入力X,クエリQに対するScaledDotProductAttentionの出力は以下の式で表すことができます。

$$Attention(Q,K,V) = Attention(Q,X,X) = \alpha(Q,X)X = softmax(\frac{QX^T}{\sqrt{D}})X$$

それでは、 ScaledDotProductAttentionのPytorchによる実装を見てみましょう.

```python
import numpy as np
import torch
from torch import nn


class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k: int) -> None:
        super().__init__()
        self.d_k = d_k

    def forward(
        self,
        q: torch.Tensor,  # =Q
        k: torch.Tensor,  # =X
        v: torch.Tensor,  # =X
        mask: torch.Tensor = None,
    ) -> torch.Tensor:
        scalar = np.sqrt(self.d_k)
        attention_weight = torch.matmul(q, torch.transpose(k, 1, 2)) / scalar # 「Q * X^T / (D^0.5)」" を計算

        if mask is not None: # maskに対する処理
            if mask.dim() != attention_weight.dim():
                raise ValueError(
                    "mask.dim != attention_weight.dim, mask.dim={}, attention_weight.dim={}".format(
                        mask.dim(), attention_weight.dim()
                    )
                )
            attention_weight = attention_weight.data.masked_fill_(
                mask, -torch.finfo(torch.float).max
            ) 

        attention_weight = nn.functional.softmax(attention_weight, dim=2) # Attention Weightを計算
        return torch.matmul(attention_weight, v) # (Attention Weight) * X により重み付け.
```


## 3.3 Multihead Attention

3.2節で、TransformerモデルはAttentionの計算方法としてScaledDotProductAttentionを採用していることを説明しました。

しかし、Transformerで採用されているAttentionは単なるScaledDotProductAttentionではありません。実際のTransformerでは単一の入力に対して、**複数のScaledDotProductAttentionを並列で実行するMultihead Attention**という仕組みが採用されています。

ここで、本家の論文に掲載されているMultihead Attentionの概要図を見てみましょう。

![Multihead Attention](https://user-images.githubusercontent.com/57289763/160265954-9451fb41-3906-4f29-8e74-d4255925141c.png)

図中のh(ヘッド数)は並列実行するScaledDotProductAttentionの数を表します.

Multihead Attentionでは、以下のような処理が行われます。
1. Attention層に対する入力$Q$($N_{Q}$$\times$$d_{model}$),$K$($N$$\times$$d_{model}$),$V$($N$$\times$$d_{model}$)を$h$(ヘッド数)の数だけ複製する
2. 複製した入力$Q_i$,$K_i$,$V_i$(i=1~h)を行列$W_i^q$($d_{model}$$\times$$d_k$),$W_i^k$($d_{model}$$\times$$d_k$),$W_i^v$($d_{model}$$\times$$d_v$)により、$d_{model}$→$d_v$,$d_k$へと線形変換する。
3. そうして得られた$Q_i$$W_i^q$($N_Q$$\times$$d_k$),$K_i$$W^k_i$($N$$\times$$d_k$),$V_i$$W^v_i$($N$$\times$$d_v$)をh個存在するScaledDotProductAttentionへ入力する
4. 並列実行されたScaledDotProductAttentionから得られるh個の出力head(i=1~h,$N$$\times$$d_v$)を結合(concat)し、行列$O$($N$$\times$$hd_v$)を得る。
5. $O$$W^O$により$O$を$hd_v$→$d_{model}$に線形変換し、得られた値が最終的な出力となる。

定式化すると以下の通りになります。


$$ head_i = ScaledDotProductAttention(Q_iW^q_i,K_iW^k_i,V_iW^v_i) (i = 1 \sim h)$$
$$ O = Concat(head_1, ..., head_h) $$
$$ MultiHead(Q,K,V) = OW^O $$


それでは、以上を踏まえて、実装を見ていきましょう.
```python
import torch
from layers.transformer.ScaledDotProductAttention import ScaledDotProductAttention
from torch import nn


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, h: int) -> None:
        super().__init__()
        self.d_model = d_model
        self.h = h
        self.d_k = d_model // h
        self.d_v = d_model // h

        #
        self.W_k = nn.Parameter(
            torch.Tensor(h, d_model, self.d_k)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)
        )

        self.W_q = nn.Parameter(
            torch.Tensor(h, d_model, self.d_k)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)
        )

        self.W_v = nn.Parameter(
            torch.Tensor(h, d_model, self.d_v)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)
        )

        self.scaled_dot_product_attention = ScaledDotProductAttention(self.d_k)

        self.linear = nn.Linear(h * self.d_v, d_model)

    def forward(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        mask_3d: torch.Tensor = None,
    ) -> torch.Tensor:

        batch_size, seq_len = q.size(0), q.size(1)

        """repeat Query,Key,Value by num of heads"""
        q = q.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model
        k = k.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model
        v = v.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model

        """Linear before scaled dot product attention"""
        q = torch.einsum(
            "hijk,hkl->hijl", (q, self.W_q)
        )  # head, batch_size, d_k, seq_len
        k = torch.einsum(
            "hijk,hkl->hijl", (k, self.W_k)
        )  # head, batch_size, d_k, seq_len
        v = torch.einsum(
            "hijk,hkl->hijl", (v, self.W_v)
        )  # head, batch_size, d_k, seq_len

        """Split heads"""
        q = q.view(self.h * batch_size, seq_len, self.d_k)
        k = k.view(self.h * batch_size, seq_len, self.d_k)
        v = v.view(self.h * batch_size, seq_len, self.d_v)

        if mask_3d is not None:
            mask_3d = mask_3d.repeat(self.h, 1, 1)

        """Scaled dot product attention"""
        attention_output = self.scaled_dot_product_attention(
            q, k, v, mask_3d
        )  # (head*batch_size, seq_len, d_model)

        attention_output = torch.chunk(attention_output, self.h, dim=0)
        attention_output = torch.cat(attention_output, dim=2)

        """Linear after scaled dot product attention"""
        output = self.linear(attention_output)
        return output
```


## 3.4 PositionalEncoding

## 3.5 Position-wise Feed-Forward Networks

## 3.6 Encoder
## 3.7 Decoder
## 3.8 Transformer(完成版)
## .英語 → 日本語翻訳機の学習

さて、モデルの実装が完了したので、いよいよモデルの学習に移ります。モデルを学習するにあたって、モデルの学習を目的としたクラス・Trainerを[train.py](https://github.com/YadaYuki/en_ja_translator_pytorch/blob/master/train.py#L22)に定義しました。なお、TrainerクラスはPytorch LightningのAPIを参考にしており、以下の5つのメソッドを持ちます。

- loss_fn: 誤差関数の計算
- train_step: バッチ学習における1ステップ(訓練)
- val_step: バッチ学習における1ステップ(検証)
- fit: バッチ学習によるモデルの訓練・検証を行う
- test: テストデータによるモデルの検証を行う

というわけで、実装を見てみましょう。Trainerクラスの実装は以下の通りです(それなりに量があるためアコーディオンにしてあります)

:::details Trainerクラス
```python
from os.path import join
from typing import List, Tuple

import torch
from matplotlib import pyplot as plt
from torch import nn, optim
from torch.utils.data import DataLoader

from const.path import (
    FIGURE_PATH,
    KFTT_TOK_CORPUS_PATH,
    NN_MODEL_PICKLES_PATH,
    TANAKA_CORPUS_PATH,
)
from models import Transformer
from utils.dataset.Dataset import KfttDataset
from utils.evaluation.bleu import BleuScore
from utils.text.text import tensor_to_text, text_to_tensor
from utils.text.vocab import get_vocab


class Trainer:
    def __init__(
        self,
        net: nn.Module,
        optimizer: optim.Optimizer,
        critetion: nn.Module,
        bleu_score: BleuScore,
        device: torch.device,
    ) -> None:
        self.net = net
        self.optimizer = optimizer
        self.critetion = critetion
        self.device = device
        self.bleu_score = bleu_score
        self.net = self.net.to(self.device)

    def loss_fn(self, preds: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        return self.critetion(preds, labels)

    def train_step(
        self, src: torch.Tensor, tgt: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, float]:
        self.net.train()
        output = self.net(src, tgt)

        tgt = tgt[:, 1:]  # decoderからの出力は1 ~ max_lenまでなので、0以降のデータで誤差関数を計算する
        output = output[:, :-1, :]  #

        # calculate loss
        loss = self.loss_fn(
            output.contiguous().view(
                -1,
                output.size(-1),
            ),
            tgt.contiguous().view(-1),
        )

        # calculate bleu score
        _, output_ids = torch.max(output, dim=-1)
        bleu_score = self.bleu_score(tgt, output_ids)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss, output, bleu_score

    def val_step(
        self, src: torch.Tensor, tgt: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, float]:
        self.net.eval()
        output = self.net(src, tgt)

        tgt = tgt[:, 1:]
        output = output[:, :-1, :]  #

        loss = self.loss_fn(
            output.contiguous().view(
                -1,
                output.size(-1),
            ),
            tgt.contiguous().view(-1),
        )
        _, output_ids = torch.max(output, dim=-1)
        bleu_score = self.bleu_score(tgt, output_ids)

        return loss, output, bleu_score

    def fit(
        self, train_loader: DataLoader, val_loader: DataLoader, print_log: bool = True
    ) -> Tuple[List[float], List[float], List[float], List[float]]:
        # train
        train_losses: List[float] = []
        train_bleu_scores: List[float] = []
        if print_log:
            print(f"{'-'*20 + 'Train' + '-'*20} \n")
        for i, (src, tgt) in enumerate(train_loader):
            src = src.to(self.device)
            tgt = tgt.to(self.device)
            loss, _, bleu_score = self.train_step(src, tgt)
            src = src.to("cpu")
            tgt = tgt.to("cpu")

            if print_log:
                print(
                    f"train loss: {loss.item()}, bleu score: {bleu_score},"
                    + f"iter: {i+1}/{len(train_loader)} \n"
                )

            train_losses.append(loss.item())
            train_bleu_scores.append(bleu_score)

        # validation
        val_losses: List[float] = []
        val_bleu_scores: List[float] = []
        if print_log:
            print(f"{'-'*20 + 'Validation' + '-'*20} \n")
        for i, (src, tgt) in enumerate(val_loader):
            src = src.to(self.device)
            tgt = tgt.to(self.device)
            loss, _, bleu_score = self.val_step(src, tgt)
            src = src.to("cpu")
            tgt = tgt.to("cpu")

            if print_log:
                print(f"train loss: {loss.item()}, iter: {i+1}/{len(val_loader)} \n")

            val_losses.append(loss.item())
            val_bleu_scores.append(bleu_score)

        return train_losses, train_bleu_scores, val_losses, val_bleu_scores

    def test(self, test_data_loader: DataLoader) -> Tuple[List[float], List[float]]:
        test_losses: List[float] = []
        test_bleu_scores: List[float] = []
        for i, (src, tgt) in enumerate(test_data_loader):
            src = src.to(self.device)
            tgt = tgt.to(self.device)
            loss, _, bleu_score = trainer.val_step(src, tgt)
            src = src.to("cpu")
            tgt = tgt.to("cpu")

            test_losses.append(loss.item())
            test_bleu_scores.append(bleu_score)

        return test_losses, test_bleu_scores
```
:::

それでは、モデルを学習していきます。モデルの学習には以下のコマンドを実行します。

```
$ poetry run python train.py
```

上のコマンドを実行すると、以下のような出力が得られます.

```
$ poetry run python train.py

epoch: 1
--------------------Train--------------------

train loss: 10.104473114013672, bleu score: 0.0,iter: 1/4403

train loss: 9.551202774047852, bleu score: 0.0,iter: 2/4403

train loss: 8.950608253479004, bleu score: 0.0,iter: 3/4403

train loss: 8.688143730163574, bleu score: 0.0,iter: 4/4403

train loss: 8.4220552444458, bleu score: 0.0,iter: 5/4403

train loss: 8.243291854858398, bleu score: 0.0,iter: 6/4403

train loss: 8.187620162963867, bleu score: 0.0,iter: 7/4403

train loss: 7.6360859870910645, bleu score: 0.0,iter: 8/4403

....
```

以下は1ステップ当たりの訓練ロスの推移をプロットしたグラフです。

![image](https://user-images.githubusercontent.com/57289763/160236001-929f9221-12bd-464e-9c0e-29222228cd89.png)

実行環境の都合で、「モデルを十分に訓練し、テストデータに対するBleuスコア・Lossで検証を行う」という検証はできませんでしたが(すみません🙇‍♂️)、1ステップあたりの訓練ロスが順調に減っていることから、正しく学習できていると推察されます。

## .まとめ
## .参考文献

https://arxiv.org/abs/1706.03762

https://www.amazon.co.jp/%E8%A9%B3%E8%A7%A3%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0-TensorFlow-Keras%E3%83%BBPyTorch%E3%81%AB%E3%82%88%E3%82%8B%E6%99%82%E7%B3%BB%E5%88%97%E3%83%87%E3%83%BC%E3%82%BF%E5%87%A6%E7%90%86-Compass-Books%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA/dp/4839969515

https://www.amazon.co.jp/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92-%E6%94%B9%E8%A8%82%E7%AC%AC2%E7%89%88-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E5%B2%A1%E8%B0%B7-%E8%B2%B4%E4%B9%8B/dp/4065133327

https://www.amazon.co.jp/PyTorch%E5%AE%9F%E8%B7%B5%E5%85%A5%E9%96%80-Eli-Stevens/dp/4839974691

https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E7%B7%A8-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873118360/ref=pd_bxgy_img_1/358-0651022-5160614?pd_rd_w=JO4Kw&pf_rd_p=020fee25-8ced-4191-bce3-27e7ce0c0e3b&pf_rd_r=WM1R7J4578P1B0MCNSJE&pd_rd_r=2b8a68ac-2514-4675-9132-acafd1cf2853&pd_rd_wg=kDhDj&pd_rd_i=4873118360&psc=1

https://qiita.com/halhorn/items/c91497522be27bde17ce

https://deeplearning.hatenablog.com/entry/transformer

