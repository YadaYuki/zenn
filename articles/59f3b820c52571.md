---
title: "PyTorchで自作して理解するTransformer"
emoji: "🙊"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["python","pytorch","機械学習","transformer"]
published: false
---



## 1. はじめに
Transformerは2017年に「Attention is all you need」という論文で発表され、自然言語処理界にブレイクスルーを巻き起こした深層学習モデルです。論文内では、英語→ドイツ語翻訳・英語→フランス語翻訳という二つの機械翻訳タスクによる性能評価が行われています。それまで最も高い精度を出すとされていたCNN,RNN(LSTM)ベースの機械翻訳と比較して、

- 精度(Bleuスコア)
- 訓練にかかるコストの少なさ

という両方の面で、Transformerはそれらの性能を上回りました。以降、Transformerをベースとした様々なモデルが提案されています。その例としては、BERT,XLNet,GPT-3といった近年のSoTAとされているモデルが挙げられます。

Transformerは自然言語処理のみならず、他の分野でも用いられる汎用性の高いモデルです。そのため、主要なディープラーニングフレームワークである[Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)・[Tensorflow](https://tensorflow.github.io/tensor2tensor/#language-modeling)のいずれにも既に公式実装が存在しており、研究等を目的に実装する際はこれらを用いるのが一般的かと思います。

しかし、作ることは理解することへの近道。ということで、今回は取り組んだのはTransformerとTransformerを構成する層のスクラッチ実装です。本記事では、Transformerモデルを構成する各レイヤの理論的背景およびPytorchによる実装を紹介していきます。以下は実装したリポジトリになります。

https://github.com/YadaYuki/en_ja_translator_pytorch
 
## 2. Transformerとは？
<!-- Encoder DecoderEncoder Decoder -->

<!-- TODO: 実装を見ていくという旨を書く	 -->
## 3. ディレクトリ構成概観

細部に注目する前に、プロジェクトの全体像を概観することは非常に重要です。ということで、ディレクトリ構成は以下の通りになります。

```
.
├── const
│   └── path.py
├── corpus
│   └── kftt-data-1.0
├── figure
├── layers
│   └── transformer
│       ├── Embedding.py
│       ├── FFN.py
│       ├── MultiHeadAttention.py
│       ├── PositionalEncoding.py
│       ├── ScaledDotProductAttention.py
│       ├── TransformerDecoder.py
│       └── TransformerEncoder.py
├── models
│   ├── Transformer.py
│   └── __init__.py
├── mypy.ini
├── pickles
│   └── nn/
├── poetry.lock
├── poetry.toml
├── pyproject.toml
├── tests
│   ├── conftest.py
│   ├── layers/
│   ├── models/
│   └── utils/
├── train.py
└── utils
    ├── dataset/
    ├── download.py
    ├── evaluation/
    └── text/
```

`models`以下に「Transformer本体」、`layers`以下に「Transformerを構成するレイヤー」という次章で説明する実装に該当するファイルが存在します。

また、`poetry.*`という名前のファイルが存在することからわかる通り、ライブラリのパッケージ管理にはPoetryを採用しました。Poetryには、現時点で僕が把握しているだけでも、

- `poetry install`コマンド一つで必要なライブラリを全てインストールし、仮想環境を作成することができる.
- 開発時のみ必要なライブラリと開発時と本番環境の両方で必要なライブラリを一つのファイル(pyproject.toml)で定義できる.

などのメリットがあります.(Python版のnpm・yarnのようなイメージです)

## 4. Transformerを構成する層
## 4.1 Attention

Transformerの最大の特徴は**RNNのように訓練時の再帰計算を行わずに、Attention層を用いている点**です。「Attention is all you need」という論文のタイトルが物語っているように、AttentionはTransformerの中で最も重要な層です.ここでは、Transformerの核となる層であるAttention層について説明します。

Attention層の目的は、入力ベクトルの各要素の重要度を算出し、それによって入力ベクトルを重み付けることです。Attention層では、**画像やテキストといった入力データのベクトルの中で、正しい出力を得るために重要な要素はどれであるか？という重要度の役割を果たすAttention Weightという値を算出します**。「入力ベクトル」と「算出したAttention Weight」の積を計算することにより、**入力ベクトルの中で正解ラベルを得る上で特に重要な要素を重み付け**ることが可能です。

Attentionを定式化してみましょう。まずは、重要度を表すAttention Weightの計算です。Attention Weightを求める関数を用いて入力XとQ(クエリ)から、 Xの重要度を表すattention_weightを求めます。

$$ここにしき $$

Q,K,Vを用いて

$$ここにしき $$

さて、本節では、Attention層が行う演算の概要とその目的について説明しました。しかし肝心なのは、入力の重要度を表すAttention Weightをどのように算出すべきであるか？という点です(関数$\alpha$が何であるか？)。

TransformerではQ(クエリ),入力データXの内積を計算することによって、Attention Weightを算出しています。そのようなAttention層をScaledDotProductAttentionと呼びます。

## 4.2 ScaledDotProductAttention

本節はTransformerで用いられているAttention層であるScaledDotProductAttentionの理論と実装に関する説明です.

ScaledDotProductAttentionでは、クエリQ(N$\times$D)と入力X(N$\times$D)を用いて、以下のような式で表すことができます.


質問応答や機械翻訳といったタスクにTransformerモデルで取り組んでいる場合、上の式におけるQ,Xはそれぞれ文章データを行列で表現したものです。N個の単語を持つ文章を、D次元の単語分散表現で表現したそれぞれ、N$\times$Dというサイズを持つ行列となっています. 


そのため、この演算では、クエリQ内の各単語の分散表現と入力データX内の各単語の分散表現の内積を算出している、ということです。ベクトル同士の内積が大きいということは、向いている方向が近い、すなわち、ベクトル同士の類似度が高いということです。つまり、Q,X内の単語同士の類似度を関連性としていると解釈することが可能です。

上で求めたAttention WeightにXをかけるt

それでは、 ScaledDotProductAttentionのPytorchによる実装を見てみましょう.

```
import numpy as np
import torch
from torch import nn


class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k: int) -> None:
        super().__init__()
        self.d_k = d_k

    def forward(
        self,
        q: torch.Tensor,  # =Q
        k: torch.Tensor,  # =X
        v: torch.Tensor,  # =X
        mask: torch.Tensor = None,
    ) -> torch.Tensor:
        scalar = np.sqrt(self.d_k)
        attention_weight = torch.matmul(q, torch.transpose(k, 1, 2)) / scalar # 「Q * X^T / (D^0.5)」" を計算

        if mask is not None: # maskに対する処理
            if mask.dim() != attention_weight.dim():
                raise ValueError(
                    "mask.dim != attention_weight.dim, mask.dim={}, attention_weight.dim={}".format(
                        mask.dim(), attention_weight.dim()
                    )
                )
            attention_weight = attention_weight.data.masked_fill_(
                mask, -torch.finfo(torch.float).max
            ) 

        attention_weight = nn.functional.softmax(attention_weight, dim=2) # Attention Weightを計算
        return torch.matmul(attention_weight, v) # (Attention Weight) * X により重み付け.
```


## 4.3 Multihead Attention

4.2節で、TransformerモデルはAttentionの計算方法としてScaledDotProductAttentionを採用していることを説明しました。

しかし、Transformerで採用されているAttentionは単なるScaledDotProductAttentionではありません。実際のTransformerでは単一の入力に対して、**複数のScaledDotProductAttentionを並列で実行するMultihead Attention**という仕組みが採用されています。

ここで、本家の論文に掲載されているMultihead Attentionの概要図を見てみましょう。

![Multihead Attention](https://user-images.githubusercontent.com/57289763/160265954-9451fb41-3906-4f29-8e74-d4255925141c.png)

図中のh(ヘッド数)は並列実行するScaledDotProductAttentionの数を表します.

Multihead Attentionでは、以下のような処理が行われます。
1. Attention層に対する入力$Q$($N_{Q}$$\times$$d_{model}$),$K$($N$$\times$$d_{model}$),$V$($N$$\times$$d_{model}$)を$h$(ヘッド数)の数だけ複製する
2. 複製した入力$Q_i$,$K_i$,$V_i$(i=1~h)を行列$W_i^q$($d_{model}$$\times$$d_k$),$W_i^k$($d_{model}$$\times$$d_k$),$W_i^v$($d_{model}$$\times$$d_v$)により、$d_{model}$→$d_v$,$d_k$へと線形変換する。
3. そうして得られた$Q_i$$W_i^q$($N_Q$$\times$$d_k$),$K_i$$W^k_i$($N$$\times$$d_k$),$V_i$$W^v_i$($N$$\times$$d_v$)をh個存在するScaledDotProductAttentionへ入力する
4. 並列実行されたScaledDotProductAttentionから得られるh個の出力head(i=1~h,$N$$\times$$d_v$)を結合(concat)し、行列$O$($N$$\times$$hd_v$)を得る。
5. $O$$W^O$により$O$を$hd_v$→$d_{model}$に線形変換し、得られた値が最終的な出力となる。

定式化すると以下の通りになります。


$$ head_i = ScaledDotProductAttention(Q_iW^q_i,K_iW^k_i,V_iW^v_i) (i = 1 \sim h)$$
$$ O = Concat(head_1, ..., head_h) $$
$$ MultiHead(Q,K,V) = OW^O $$




それでは、以上を踏まえて、実装を見ていきましょう.
```
import torch
from layers.transformer.ScaledDotProductAttention import ScaledDotProductAttention
from torch import nn


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, h: int) -> None:
        super().__init__()
        self.d_model = d_model
        self.h = h
        self.d_k = d_model // h
        self.d_v = d_model // h

        #
        self.W_k = nn.Parameter(
            torch.Tensor(h, d_model, self.d_k)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)
        )

        self.W_q = nn.Parameter(
            torch.Tensor(h, d_model, self.d_k)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)
        )

        self.W_v = nn.Parameter(
            torch.Tensor(h, d_model, self.d_v)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)
        )

        self.scaled_dot_product_attention = ScaledDotProductAttention(self.d_k)

        self.linear = nn.Linear(h * self.d_v, d_model)

    def forward(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        mask_3d: torch.Tensor = None,
    ) -> torch.Tensor:

        batch_size, seq_len = q.size(0), q.size(1)

        """repeat Query,Key,Value by num of heads"""
        q = q.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model
        k = k.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model
        v = v.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model

        """Linear before scaled dot product attention"""
        q = torch.einsum(
            "hijk,hkl->hijl", (q, self.W_q)
        )  # head, batch_size, d_k, seq_len
        k = torch.einsum(
            "hijk,hkl->hijl", (k, self.W_k)
        )  # head, batch_size, d_k, seq_len
        v = torch.einsum(
            "hijk,hkl->hijl", (v, self.W_v)
        )  # head, batch_size, d_k, seq_len

        """Split heads"""
        q = q.view(self.h * batch_size, seq_len, self.d_k)
        k = k.view(self.h * batch_size, seq_len, self.d_k)
        v = v.view(self.h * batch_size, seq_len, self.d_v)

        if mask_3d is not None:
            mask_3d = mask_3d.repeat(self.h, 1, 1)

        """Scaled dot product attention"""
        attention_output = self.scaled_dot_product_attention(
            q, k, v, mask_3d
        )  # (head*batch_size, seq_len, d_model)

        attention_output = torch.chunk(attention_output, self.h, dim=0)
        attention_output = torch.cat(attention_output, dim=2)

        """Linear after scaled dot product attention"""
        output = self.linear(attention_output)
        return output
```


## 4.4 PositionalEncoding

## 4.5 Position-wise Feed-Forward Networks

## 4.6 Encoder
## 4.7 Decoder
## 4.8 Transformer(完成版)
## 5.英語 → 日本語翻訳機の学習



```
```

![image](https://user-images.githubusercontent.com/57289763/160236001-929f9221-12bd-464e-9c0e-29222228cd89.png)

## 6.まとめ
## 7.参考文献

https://arxiv.org/abs/1706.03762

https://www.amazon.co.jp/%E8%A9%B3%E8%A7%A3%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0-TensorFlow-Keras%E3%83%BBPyTorch%E3%81%AB%E3%82%88%E3%82%8B%E6%99%82%E7%B3%BB%E5%88%97%E3%83%87%E3%83%BC%E3%82%BF%E5%87%A6%E7%90%86-Compass-Books%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA/dp/4839969515

https://www.amazon.co.jp/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92-%E6%94%B9%E8%A8%82%E7%AC%AC2%E7%89%88-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E5%B2%A1%E8%B0%B7-%E8%B2%B4%E4%B9%8B/dp/4065133327

https://www.amazon.co.jp/PyTorch%E5%AE%9F%E8%B7%B5%E5%85%A5%E9%96%80-Eli-Stevens/dp/4839974691

https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E7%B7%A8-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873118360/ref=pd_bxgy_img_1/358-0651022-5160614?pd_rd_w=JO4Kw&pf_rd_p=020fee25-8ced-4191-bce3-27e7ce0c0e3b&pf_rd_r=WM1R7J4578P1B0MCNSJE&pd_rd_r=2b8a68ac-2514-4675-9132-acafd1cf2853&pd_rd_wg=kDhDj&pd_rd_i=4873118360&psc=1

https://qiita.com/halhorn/items/c91497522be27bde17ce

https://deeplearning.hatenablog.com/entry/transformer

